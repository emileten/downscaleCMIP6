{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151b3d1c-fbb2-4764-ae15-ab80f4cca63c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rechunk, cap, Q/A, and deliver precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ff668b-94c6-443e-a334-5abd5519bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = '''\n",
    "v1.1 : initial release (version number set to match temperature). \n",
    "'''.strip()\n",
    "\n",
    "OUTPUT_VERSION = '1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024290a8-ef86-42a0-8645-57531ea2229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fsspec\n",
    "import requests\n",
    "import contextlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zarr\n",
    "import rechunker\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import dask.distributed as dd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed17d4e5-9951-495a-9da5-7072691c1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/repositories/downscaleCMIP6/notebooks/downscaling_pipeline\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6adcc35-05c6-4978-b9c3-20ce106d7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIVERY_MODELS = [\n",
    "    #'BCC-CSM2-MR',\n",
    "    #'FGOALS-g3',\n",
    "    #'ACCESS-ESM1-5',\n",
    "    #'ACCESS-CM2',\n",
    "    #'INM-CM4-8',\n",
    "    #'INM-CM5-0',\n",
    "    #'MIROC-ES2L',\n",
    "    #'MIROC6',\n",
    "    'NorESM2-LM',\n",
    "    #'NorESM2-MM',\n",
    "    'GFDL-ESM4',\n",
    "    #'GFDL-CM4',\n",
    "    #'NESM3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120fd3e6-dfbf-4f12-914b-eaee6da083b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTITUTIONS = {    \n",
    "    'BCC-CSM2-MR': 'BCC',\n",
    "    'FGOALS-g3': 'CAS',\n",
    "    'ACCESS-ESM1-5': 'CSIRO',\n",
    "    'ACCESS-CM2': 'CSIRO-ARCCSS',\n",
    "    'INM-CM4-8': 'INM',\n",
    "    'INM-CM5-0': 'INM',\n",
    "    'MIROC-ES2L': 'MIROC',\n",
    "    'MIROC6': 'MIROC',\n",
    "    'NorESM2-LM': 'NCC',\n",
    "    'NorESM2-MM': 'NCC',\n",
    "    'GFDL-ESM4': 'NOAA-GFDL',\n",
    "    'GFDL-CM4': 'NOAA-GFDL',\n",
    "    'NESM3': 'NUIST',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bd7ffa-9fe8-4fb1-8572-26f9f77e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_MEMBERS = {\n",
    "    'BCC-CSM2-MR': 'r1i1p1f1',\n",
    "    'FGOALS-g3': 'r1i1p1f1',\n",
    "    'ACCESS-ESM1-5': 'r1i1p1f1',\n",
    "    'ACCESS-CM2': 'r1i1p1f1',\n",
    "    'INM-CM4-8': 'r1i1p1f1',\n",
    "    'INM-CM5-0': 'r1i1p1f1',\n",
    "    'MIROC-ES2L': 'r1i1p1f2',\n",
    "    'MIROC6': 'r1i1p1f1',\n",
    "    'NorESM2-LM': 'r1i1p1f1',\n",
    "    'NorESM2-MM': 'r1i1p1f1',\n",
    "    'GFDL-ESM4': 'r1i1p1f1',\n",
    "    'GFDL-CM4': 'r1i1p1f1',\n",
    "    'NESM3': 'r1i1p1f1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a368f9f-3a8e-4b8e-b1c4-e9cebbdd7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SPECS = {\n",
    "    \"ACCESS-CM2\": \"gn\",\n",
    "    \"MRI-ESM2-0\": \"gn\",\n",
    "    \"CanESM5\": \"gn\",\n",
    "    \"ACCESS-ESM1-5\": \"gn\",\n",
    "    \"MIROC6\": \"gn\",\n",
    "    \"EC-Earth3\": \"gr\",\n",
    "    \"EC-Earth3-Veg-LR\": \"gr\",\n",
    "    \"EC-Earth3-Veg\": \"gr\",\n",
    "    \"MPI-ESM1-2-HR\": \"gn\",\n",
    "    \"CMCC-ESM2\": \"gn\",\n",
    "    \"INM-CM5-0\": \"gr1\",\n",
    "    \"INM-CM4-8\": \"gr1\",\n",
    "    \"MIROC-ES2L\": \"gn\",\n",
    "    \"MPI-ESM1-2-LR\": \"gn\",\n",
    "    \"FGOALS-g3\": \"gn\",\n",
    "    \"BCC-CSM2-MR\": \"gn\",\n",
    "    \"AWI-CM-1-1-MR\": \"gn\",\n",
    "    \"NorESM2-LM\": \"gn\",\n",
    "    \"GFDL-ESM4\": \"gr1\",\n",
    "    \"GFDL-CM4\": \"gr1\",\n",
    "    \"CAMS-CSM1-0\": \"gn\",\n",
    "    \"NorESM2-MM\": \"gn\",\n",
    "    \"NESM3\": \"gn\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00ae1c1-d790-43d5-a023-46ad09eb6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_EXTENSION_SCENARIO = {\n",
    "    \"ACCESS-CM2\": \"ssp370\",\n",
    "    \"MRI-ESM2-0\": \"ssp370\",\n",
    "    \"CanESM5\": \"ssp370\",\n",
    "    \"ACCESS-ESM1-5\": \"ssp370\",\n",
    "    \"MIROC6\": \"ssp370\",\n",
    "    \"EC-Earth3\": \"ssp370\",\n",
    "    \"EC-Earth3-Veg-LR\": \"ssp370\",\n",
    "    \"EC-Earth3-Veg\": \"ssp370\",\n",
    "    \"MPI-ESM1-2-HR\": \"ssp370\",\n",
    "    \"CMCC-ESM2\": \"ssp370\",\n",
    "    \"INM-CM5-0\": \"ssp370\",\n",
    "    \"INM-CM4-8\": \"ssp370\",\n",
    "    \"MIROC-ES2L\": \"ssp370\",\n",
    "    \"MPI-ESM1-2-LR\": \"ssp370\",\n",
    "    \"FGOALS-g3\": \"ssp370\",\n",
    "    \"BCC-CSM2-MR\": \"ssp370\",\n",
    "    \"AWI-CM-1-1-MR\": \"ssp370\",\n",
    "    \"NorESM2-LM\": \"ssp370\",\n",
    "    \"GFDL-ESM4\": \"ssp370\",\n",
    "    \"GFDL-CM4\": \"ssp245\",\n",
    "    \"CAMS-CSM1-0\": \"ssp370\",\n",
    "    \"NorESM2-MM\": \"ssp370\",\n",
    "    \"NESM3\": \"ssp245\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6009ee24-ee82-4727-9402-efae0be145e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_REF_0p25deg_FP = 'gs://support-c23ff1a3/qplad-fine-reference/pr/v20220201000555.zarr'\n",
    "\n",
    "cleaned_gcm_pattern = (\n",
    "    'gs://clean-b1dbca25/cmip6/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{source_version}.zarr'\n",
    ")\n",
    "\n",
    "downscaled_filepatt = (\n",
    "    'gs://downscaled-288ec5ac/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}.zarr'\n",
    ")\n",
    "\n",
    "rechunked_temp_store_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked-temp-store.zarr'\n",
    ")\n",
    "\n",
    "rechunked_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked.zarr'\n",
    ")\n",
    "\n",
    "capped_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-pr-capped.zarr'\n",
    ")\n",
    "\n",
    "OUTPUT_PATTERN = (\n",
    "    'gs://downscaled-288ec5ac/outputs/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{delivery_version}.zarr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80480c37-686e-469e-ad8c-c80c9fda8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs', token='/opt/gcsfuse_tokens/impactlab-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823eec05-0206-44b8-93e4-f8f49e9a32ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36cabad99c7744adba69a98407df11d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tasmax:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precip_spec_file = f'version_specs/precip_{OUTPUT_VERSION}.json'\n",
    "\n",
    "if os.path.isfile(precip_spec_file):\n",
    "    with open(precip_spec_file, 'r') as f:\n",
    "        INPUT_FILE_VERSIONS = json.load(f)\n",
    "\n",
    "else:\n",
    "    pr_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "    for m in tqdm(DELIVERY_MODELS, desc='tasmax'):\n",
    "\n",
    "        inst = INSTITUTIONS[m]\n",
    "\n",
    "        for act, scen in [\n",
    "            ('CMIP', 'historical'),\n",
    "            ('ScenarioMIP', 'ssp245'),\n",
    "            ('ScenarioMIP', 'ssp370'),\n",
    "        ]:\n",
    "            pr_fps[m][scen] = list(\n",
    "                fs.glob(\n",
    "                    downscaled_filepatt.format(\n",
    "                        activity_id=act,\n",
    "                        institution_id=inst,\n",
    "                        source_id=m,\n",
    "                        experiment_id=scen,\n",
    "                        member_id=ENSEMBLE_MEMBERS[m],\n",
    "                        table_id='day',\n",
    "                        variable_id='pr',\n",
    "                        grid_spec=GRID_SPECS[m],\n",
    "                        run_version='*',\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    pr_max_versions = {\n",
    "        m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "        for m, mspec in pr_fps.items()\n",
    "    }\n",
    "\n",
    "    INPUT_FILE_VERSIONS = {\n",
    "        'version': OUTPUT_VERSION,\n",
    "        'created': pd.Timestamp.now(tz='US/Pacific').strftime('%c'),\n",
    "        'history': HISTORY,\n",
    "        'file_paths': {\n",
    "            'pr': pr_max_versions,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ! mkdir -p version_specs\n",
    "\n",
    "    with open(precip_spec_file, 'w') as f:\n",
    "        f.write(json.dumps(INPUT_FILE_VERSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce4d86a-7b93-4414-b948-e2917c4ae4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    for v in ['pr']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            raise ValueError(f\"model {m} not found for {v}\")\n",
    "\n",
    "        for s, fp in INPUT_FILE_VERSIONS['file_paths'][v][m].items():\n",
    "            assert m in fp, f\"model name '{m}' not found in filepath '{fp}'\"\n",
    "            assert s in fp, f\"scenario '{s}' not found in filepath '{fp}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "970159e0-3885-4788-8005-deacad19aaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['CRS_SUPPORT_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36a0b37e-8972-4b79-a4f9-ec915ad39b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGOALS-g3\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "BCC-CSM2-MR\n",
      "ACCESS-ESM1-5\n",
      "ACCESS-CM2\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "NorESM2-MM\n",
      "GFDL-CM4\n",
      "NESM3\n"
     ]
    }
   ],
   "source": [
    "CC0_LICENSE_MODELS = ['FGOALS-g3', 'INM-CM4-8', 'INM-CM5-0']\n",
    "\n",
    "CC_BY_LICENSE_MODELS = [\n",
    "    'BCC-CSM2-MR',\n",
    "    'ACCESS-ESM1-5',\n",
    "    'ACCESS-CM2',\n",
    "    'MIROC-ES2L',\n",
    "    'MIROC6',\n",
    "    'NorESM2-LM',\n",
    "    'NorESM2-MM',\n",
    "    'GFDL-CM4',\n",
    "    'GFDL-ESM4',\n",
    "    'NESM3',\n",
    "]\n",
    "\n",
    "for m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS):\n",
    "    for v in ['pr']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            print(m)\n",
    "\n",
    "for m in DELIVERY_MODELS:\n",
    "    assert m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "396955bd-c222-44d3-a42e-e5a071cd464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    hist_extension = HIST_EXTENSION_SCENARIO[m]\n",
    "    if len(INPUT_FILE_VERSIONS['file_paths']['pr'][m]) == 0:\n",
    "        continue\n",
    "\n",
    "    assert hist_extension in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys(), (\n",
    "        f\"{hist_extension} not in {INPUT_FILE_VERSIONS['file_paths']['pr'][m]} for model {m}\"\n",
    "    )\n",
    "\n",
    "    if hist_extension != 'ssp370':\n",
    "        assert 'ssp370' not in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys()    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7b4dd-9494-4c93-859a-dfe3b99380d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029cafa-e4d0-49e8-957f-329a06ba1619",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87996b88-1934-4d80-931d-f486b3f44a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'downscaled_filepatt',\n",
    "    'rechunked_temp_store_pattern',\n",
    "    'rechunked_pattern',\n",
    "    'capped_pattern',\n",
    "    'OUTPUT_PATTERN',\n",
    "])\n",
    "def get_spec_from_input_fp(fp, output_version=OUTPUT_VERSION):\n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        grid,\n",
    "        run_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    spec = dict(\n",
    "        bucket=bucket,\n",
    "        stage=stage,\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        grid=grid,\n",
    "        run_version=run_version,\n",
    "    )\n",
    "\n",
    "    for (name, fpatt) in [\n",
    "        ('downscaled_fp', downscaled_filepatt),\n",
    "        ('rechunk_temp_store_fp', rechunked_temp_store_pattern),\n",
    "        ('rechunked_fp', rechunked_pattern),\n",
    "        ('capped_fp', capped_pattern),\n",
    "        ('output_fp', OUTPUT_PATTERN),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version=run_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "\n",
    "    return spec\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_spec_from_output_fp(fp, output_pattern=OUTPUT_PATTERN):\n",
    "    \n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        output_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    output_fp = output_pattern.format(\n",
    "        activity_id=activity,\n",
    "        institution_id=institution,\n",
    "        source_id=model,\n",
    "        experiment_id=scenario,\n",
    "        member_id=ensemble,\n",
    "        variable_id=variable,\n",
    "        table_id=table,\n",
    "        delivery_version=output_version,\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        output_version=output_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9d367-2108-49ba-8957-7be42927f6c1",
   "metadata": {},
   "source": [
    "## Stage 1: Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4b280a82-83d3-4cd9-bea9-3ed1598447f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'INPUT_FILE_VERSIONS',\n",
    "])\n",
    "def rechunk_data(varname, model, scenario, worker_memory_limit):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', token='/opt/gcsfuse_tokens/impactlab-data.json', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    target_chunks = {\n",
    "        varname: {'time': 365, 'lat': 360, 'lon': 360},\n",
    "        'time': {'time': 365},\n",
    "        'lat': {'lat': 360},\n",
    "        'lon': {'lon': 360},\n",
    "    }\n",
    "\n",
    "    input_fp = INPUT_FILE_VERSIONS['file_paths'][varname][model][scenario]\n",
    "    input_spec = get_spec_from_input_fp(input_fp)\n",
    "\n",
    "    rechunked_temp_store_fp = input_spec['rechunk_temp_store_fp']\n",
    "    rechunked_fp = input_spec['rechunked_fp']\n",
    "\n",
    "    if fs.isdir(rechunked_fp):\n",
    "        return\n",
    "\n",
    "    mapper = fs.get_mapper(input_fp)\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        rechunked_mapper = fs.get_mapper(rechunked_fp)\n",
    "\n",
    "        chunk_job = rechunker.rechunk(\n",
    "            source=ds,\n",
    "            target_chunks=target_chunks,\n",
    "            max_mem=worker_memory_limit,\n",
    "            target_store=rechunked_mapper,\n",
    "            temp_store=fs.get_mapper(rechunked_temp_store_fp),\n",
    "        )\n",
    "\n",
    "        chunk_job_persist = chunk_job._plan.persist()\n",
    "        dd.wait(chunk_job_persist)\n",
    "\n",
    "    zarr.convenience.consolidate_metadata(rechunked_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c326ce-bb80-463e-a7e7-302571abddd4",
   "metadata": {},
   "source": [
    "## Stage 2: Cap precip at the max(max)*max(max)/max(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "24f74793-15f2-4e28-81f7-5a012ae2cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['cleaned_gcm_pattern', 'HIST_EXTENSION_SCENARIO', 'INPUT_FILE_VERSIONS', 'CLEANED_REF_0p25deg_FP'])\n",
    "def cap_precip(pr_input_fp):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', token='/opt/gcsfuse_tokens/impactlab-data.json')\n",
    "\n",
    "    pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "    dest_fp = pr_spec['capped_fp']\n",
    "    if fs.isdir(dest_fp):\n",
    "        return\n",
    "\n",
    "    gcm_rechunked = xr.open_zarr(fs.get_mapper(pr_spec['rechunked_fp']))\n",
    "    \n",
    "    source_file_versions = INPUT_FILE_VERSIONS['file_paths']['pr'][pr_spec['model']]\n",
    "\n",
    "    if pr_spec['scenario'] == 'historical':\n",
    "        proj_scen = HIST_EXTENSION_SCENARIO[pr_spec['model']]\n",
    "        proj_fp = get_spec_from_input_fp(source_file_versions[proj_scen])['rechunked_fp']\n",
    "        with xr.open_zarr(fs.get_mapper(proj_fp)) as proj:\n",
    "            source_version_proj = proj.attrs['version_id']\n",
    "\n",
    "        source_version_hist = gcm_rechunked.attrs['version_id']\n",
    "        \n",
    "    else:\n",
    "        proj_scen = pr_spec['scenario']\n",
    "        hist_fp = get_spec_from_input_fp(source_file_versions['historical'])['rechunked_fp']\n",
    "        with xr.open_zarr(fs.get_mapper(hist_fp)) as hist:\n",
    "            source_version_hist = hist.attrs['version_id']\n",
    "\n",
    "        source_version_proj = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    source_version_id = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    clean_fp_hist = cleaned_gcm_pattern.format(\n",
    "        activity_id='CMIP',\n",
    "        institution_id=pr_spec['institution'],\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id='historical',\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_hist,\n",
    "    )\n",
    "\n",
    "    clean_fp_proj = cleaned_gcm_pattern.format(\n",
    "        activity_id='ScenarioMIP',\n",
    "        institution_id=pr_spec['institution'],\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id=proj_scen,\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_proj,\n",
    "    )\n",
    "\n",
    "    ref_fp = CLEANED_REF_0p25deg_FP\n",
    "\n",
    "    try:\n",
    "        clean_hist = xr.open_zarr(fs.get_mapper(clean_fp_hist))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_hist)\n",
    "\n",
    "    try:\n",
    "        clean_proj = xr.open_zarr(fs.get_mapper(clean_fp_proj))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_proj)\n",
    "\n",
    "    ref = xr.open_zarr(fs.get_mapper(ref_fp))\n",
    "\n",
    "    ref_maxpr = ref.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_hist_maxpr = clean_hist.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        xr.concat([clean_hist, clean_proj], dim='time')\n",
    "        .pr\n",
    "        .groupby('time.year')\n",
    "        .max(dim='time')\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    # convert lons to [-180, 180]\n",
    "\n",
    "    gcm_hist_maxpr = (\n",
    "        gcm_hist_maxpr\n",
    "        .assign_coords(lon=((gcm_hist_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        gcm_proj_maxpr\n",
    "        .assign_coords(lon=((gcm_proj_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "    \n",
    "    gcm_proj_maxpr_rolled = (\n",
    "        gcm_proj_maxpr\n",
    "        .rolling(year=21, center=True, min_periods=21).max(dim='year')\n",
    "    )\n",
    "\n",
    "    gcm_factor = (\n",
    "        (gcm_proj_maxpr_rolled.dropna(dim='year', how='all') / gcm_hist_maxpr)\n",
    "        .rename({'lat': 'lat_coarse', 'lon': 'lon_coarse'})\n",
    "        .sel(lat_coarse=ref_maxpr.lat, lon_coarse=ref_maxpr.lon, method='nearest')\n",
    "        .drop(['lat_coarse', 'lon_coarse'])\n",
    "    )\n",
    "\n",
    "    upper_bound = np.maximum(1, gcm_factor) * ref_maxpr\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        upper_bound_full = (\n",
    "            upper_bound\n",
    "            .reindex(year=np.unique(gcm_rechunked.time.dt.year), method='nearest')\n",
    "            .chunk({'year': 1})\n",
    "            .sel(year=gcm_rechunked.time.dt.year)\n",
    "            .drop('year')\n",
    "        )\n",
    "\n",
    "        gcm_capped = gcm_rechunked.copy(deep=False)\n",
    "\n",
    "        gcm_capped['pr'] = np.minimum(upper_bound_full, gcm_rechunked['pr'])\n",
    "        gcm_capped['pr'].attrs = gcm_rechunked['pr'].attrs\n",
    "        gcm_capped.attrs = gcm_rechunked.attrs\n",
    "\n",
    "        out_mapper = fs.get_mapper(dest_fp)\n",
    "        gcm_capped.to_zarr(out_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331144c-65e9-49fb-b339-34329b371229",
   "metadata": {},
   "source": [
    "## Stage 3: copy to destination directory & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7820315-4898-48e9-a5ef-19b8c049bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['CC0_LICENSE_MODELS', 'CC_BY_LICENSE_MODELS'])\n",
    "def quick_check_file(fp, ds, spec):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # check that metadata matches file spec\n",
    "\n",
    "    assert ds.attrs['institution_id'] == spec['institution'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['institution_id']} ≠ {spec['institution']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] == spec['model'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['source_id']} ≠ {spec['model']}\"\n",
    "    )\n",
    "\n",
    "    assert spec['activity'] in ds.attrs['activity_id'], (\n",
    "        f\"invalid attrs in {fp}: {spec['activity']} not in {ds.attrs['activity_id']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['experiment_id'] == spec['scenario'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['experiment_id']} ≠ {spec['scenario']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['variant_label'] == spec['ensemble'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['variant_label']} ≠ {spec['ensemble']}\"\n",
    "    )\n",
    "\n",
    "    if spec['variable'] == 'tasmax':\n",
    "        assert ds['tasmax'].attrs['long_name'] == 'Daily Maximum Near-Surface Air Temperature'\n",
    "        assert ds['tasmax'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'tasmin':\n",
    "        assert ds['tasmin'].attrs['long_name'] == 'Daily Minimum Near-Surface Air Temperature'\n",
    "        assert ds['tasmin'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'pr':\n",
    "        raise NotImplementedError()\n",
    "#         assert ds['tasmax'].attrs['units'] == 'mm/day'\n",
    "    else:\n",
    "        raise ValueError(f'variable not recognized: {spec[\"variable\"]}')\n",
    "\n",
    "    # Check licensing fields & endpoint URL\n",
    "\n",
    "    # check that license URL points to a real location and it exists\n",
    "    license_url = ds.attrs['license']\n",
    "    assert ds.attrs['source_id'] in license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {license_url}'\n",
    "    )\n",
    "    r = requests.get(license_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # check that \"Creaive Commons\" and the model name show up on the page\n",
    "    assert ds.attrs['source_id'] in r.text, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    assert \"Creative Commons\" in r.text, (\n",
    "        f'\"Creative Commons\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    # check that \"Creative Commons\" appears in the raw license text\n",
    "\n",
    "    raw_license_url = (\n",
    "        ds.attrs['license']\n",
    "        .replace('github.com', 'raw.githubusercontent.com')\n",
    "        .replace('/blob/', '/')\n",
    "        .replace('/tree/', '/')\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] in raw_license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {raw_license_url}'\n",
    "    )\n",
    "    r = requests.get(raw_license_url)\n",
    "    r.raise_for_status()\n",
    "    assert 'Creative Commons' in r.text, (\n",
    "        f'\"Creative Commons\" not found in license text: {raw_license_url}'\n",
    "    )\n",
    "\n",
    "    if spec['model'] in CC0_LICENSE_MODELS:\n",
    "        assert 'CC0 1.0 Universal' in r.text, (\n",
    "            f\"expected CC0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_LICENSE_MODELS:\n",
    "        assert 'Attribution 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"deploying model with unknown license: {spec['model']} at {fp}\"\n",
    "        )\n",
    "\n",
    "    # Check dimension size & membership\n",
    "\n",
    "    for c in ds.coords.keys():\n",
    "        assert ds.coords[c].notnull().all().item() is True, f\"NaNs found in coordinate '{c}' in {fp}\"\n",
    "\n",
    "    if spec['activity'] == 'ScenarioMIP':\n",
    "        date_range = xr.cftime_range(\"2015-01-01\", \"2099-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "        if len(ds.time) > len(date_range):\n",
    "            date_range = xr.cftime_range(\"2015-01-01\", \"2100-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "    else:\n",
    "        date_range = xr.cftime_range(\"1950-01-01\", \"2014-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "    assert ds.sizes['time'] == len(date_range), (\n",
    "        f\"unexpected length of dimension 'time': length {len(ds.time)}; \"\n",
    "        f\"expected {len(date_range)} in {fp}\"\n",
    "    )\n",
    "\n",
    "    assert date_range.isin(ds.time.dt.floor('D').values).all(), f\"invalid coords in {fp}\"\n",
    "\n",
    "    assert pd.Series(np.arange(-179.875, 180, 0.25)).isin(ds.lon.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "    assert pd.Series(np.arange(-89.875, 90, 0.25)).isin(ds.lat.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "\n",
    "    varnames = list(ds.data_vars.keys())\n",
    "    assert len(varnames) == 1\n",
    "    varname = varnames[0]\n",
    "\n",
    "    assert ds[varname].sizes['lat'] == 720, f\"lat not length 720 in {fp}:\\n{ds}\"\n",
    "    assert ds[varname].sizes['lon'] == 1440, f\"lon not length 1440 in {fp}:\\n{ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "738e7790-3ede-4c5a-86fc-84bd11d42746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spec = get_spec_from_input_fp(INPUT_FILE_VERSIONS['file_paths']['pr']['FGOALS-g3']['historical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0a313c89-0f35-4ddb-bc6b-975b15dd9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spec['output_fp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24ec495f-c345-47eb-beb4-f37821e754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS'])\n",
    "def validate_outputs(fp, quick=False):\n",
    "    spec = get_spec_from_output_fp(fp)\n",
    "\n",
    "    fs = fsspec.filesystem('gs', token='/opt/gcsfuse_tokens/impactlab-data.json', timeout=60, cache_timeout=60, requests_timeout=60, read_timeout=60, conn_timeout=60)\n",
    "\n",
    "    mapper = fs.get_mapper(fp)\n",
    "\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        quick_check_file(fp, ds, spec)\n",
    "\n",
    "        if quick:\n",
    "            return\n",
    "\n",
    "        # check variable contents\n",
    "\n",
    "        varnames = list(ds.data_vars.keys())\n",
    "        assert len(varnames) == 1\n",
    "        varname = varnames[0]\n",
    "\n",
    "        to_check = ds[varname].sel(lat=slice(-80, 80))\n",
    "\n",
    "        nans = to_check.isnull().any()\n",
    "        min_val = to_check.min()\n",
    "        max_val = to_check.max()\n",
    "\n",
    "        nans, vmin, vmax = dd.get_client().compute(\n",
    "            [nans, min_val, max_val],\n",
    "            optimize_graph=True,\n",
    "            sync=True,\n",
    "            retries=3,\n",
    "        )\n",
    "\n",
    "        assert nans.item() is False, f\"NaNs found in {fp}\"\n",
    "\n",
    "        if varname == 'tasmax':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'tasmin':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'pr':\n",
    "            allowed_min = 0\n",
    "            allowed_max = 3000\n",
    "        else:\n",
    "            raise ValueError(f'Variable name not recognized: {varname}\\nin file: {fp}')\n",
    "\n",
    "        assert (vmin >= allowed_min).item() is True, (\n",
    "            f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "        assert (vmax <= allowed_max).item() is True, (\n",
    "            f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def copy_and_validate(\n",
    "    source_fp,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    "):\n",
    "\n",
    "    spec = get_spec_from_input_fp(source_fp, output_version=output_version)\n",
    "    flipped_fp = spec['flipped_fp']\n",
    "    output_fp = spec['output_fp']\n",
    "    model = spec['model']\n",
    "    scenario = spec['scenario']\n",
    "\n",
    "    fs = fsspec.filesystem(\n",
    "        'gs',\n",
    "        token='/opt/gcsfuse_tokens/impactlab-data.json',\n",
    "        timeout=360,\n",
    "        cache_timeout=360,\n",
    "        requests_timeout=360, read_timeout=360, conn_timeout=360)\n",
    "\n",
    "    if fs.exists(output_fp):\n",
    "        if overwrite:\n",
    "            fs.remove(output_fp, recursive=True)\n",
    "\n",
    "        else:\n",
    "            if deep_copy_check:\n",
    "                dirs = list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps])\n",
    "                if pbar:\n",
    "                    dirs = tqdm(dirs)\n",
    "\n",
    "                for d, f in dirs:\n",
    "                    src = flipped_fp[:5] + os.path.join(d, f)\n",
    "                    dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "                    assert '..' not in dst\n",
    "                    src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                            break\n",
    "                        except (FileNotFoundError, AssertionError):\n",
    "                            if i == 4:\n",
    "                                raise\n",
    "\n",
    "                            fs.rm(dst)\n",
    "                            fs.copy(src, dst)\n",
    "\n",
    "            if check:\n",
    "                try:\n",
    "                    validate_outputs(\n",
    "                        output_fp,\n",
    "                    )\n",
    "                    return\n",
    "                except (\n",
    "                    AssertionError,\n",
    "                    FileNotFoundError,\n",
    "                    ValueError,\n",
    "                    IOError,\n",
    "                    xr.coding.times.OutOfBoundsDatetime,\n",
    "                    OverflowError,\n",
    "                ):\n",
    "                    if overwrite_on_failure:\n",
    "                        fs.rm(output_fp, recursive=True)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            elif quick_check_and_retry:\n",
    "                try:\n",
    "                    validate_outputs(output_fp, quick=True)\n",
    "                    return\n",
    "                except (\n",
    "                    OverflowError,\n",
    "                    IOError,\n",
    "                    zarr.errors.GroupNotFoundError,\n",
    "                    FileNotFoundError,\n",
    "                    AssertionError,\n",
    "                    ValueError,\n",
    "                ):\n",
    "                    pass\n",
    "\n",
    "                fs.rm(output_fp, recursive=True)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    print(f'copying:\\n\\tsrc:\\t{flipped_fp}\\n\\tdst:\\t{output_fp}')\n",
    "    fs.copy(flipped_fp, output_fp, recursive=True, batch_size=1000)\n",
    "\n",
    "    if deep_copy_check:\n",
    "        for d, f in list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps]):\n",
    "            src = flipped_fp[:5] + os.path.join(d, f)\n",
    "            dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "            assert '..' not in dst\n",
    "            src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                    break\n",
    "                except (FileNotFoundError, AssertionError):\n",
    "                    if i == 4:\n",
    "                        raise\n",
    "\n",
    "                    fs.rm(dst)\n",
    "                    fs.copy(src, dst)\n",
    "\n",
    "    if check:\n",
    "        validate_outputs(\n",
    "            output_fp,\n",
    "        )\n",
    "    elif quick_check_and_retry:\n",
    "        validate_outputs(output_fp, quick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3900e5-bcc7-44c4-ad61-8a840c4ed4f1",
   "metadata": {},
   "source": [
    "# Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d85359d-5423-4904-a0e0-df79eb486d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13e90cbc82746a281d06afe0cac04f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_giant_cluster()\n",
    "cluster.scale(60)\n",
    "\n",
    "MAX_MEM = '12GB' # for standard cluster\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fe81a-cada-4f13-afc6-051c8a6cf9bc",
   "metadata": {},
   "source": [
    "# Prepare final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59af1c52-ae9d-4280-8c37-9232885cbd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "409843c4a46d492bbf1801820d173318",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscen\u001b[39m\u001b[38;5;124m'\u001b[39m: scenario, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrechunk pr\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m---> 14\u001b[0m \u001b[43mrechunk_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscenario\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_memory_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_MEM\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_postfix({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscen\u001b[39m\u001b[38;5;124m'\u001b[39m: scenario, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcap precip\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mset(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marray.slicing.split_large_chunks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m}):\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rhg_compute_tools/utils.py:440\u001b[0m, in \u001b[0;36mblock_globals.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(obj)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36mrechunk_data\u001b[0;34m(varname, model, scenario, worker_memory_limit)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m xr\u001b[38;5;241m.\u001b[39mopen_zarr(mapper) \u001b[38;5;28;01mas\u001b[39;00m ds:\n\u001b[1;32m     27\u001b[0m     rechunked_mapper \u001b[38;5;241m=\u001b[39m fs\u001b[38;5;241m.\u001b[39mget_mapper(rechunked_fp)\n\u001b[0;32m---> 29\u001b[0m     chunk_job \u001b[38;5;241m=\u001b[39m \u001b[43mrechunker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrechunk\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_chunks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_mem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_memory_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrechunked_mapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemp_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrechunked_temp_store_fp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     chunk_job_persist \u001b[38;5;241m=\u001b[39m chunk_job\u001b[38;5;241m.\u001b[39m_plan\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     38\u001b[0m     dd\u001b[38;5;241m.\u001b[39mwait(chunk_job_persist)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rechunker/api.py:305\u001b[0m, in \u001b[0;36mrechunk\u001b[0;34m(source, target_chunks, max_mem, target_store, target_options, temp_store, temp_options, executor)\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecutor type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(executor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not supported for source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(source)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m         )\n\u001b[1;32m    296\u001b[0m copy_spec, intermediate, target \u001b[38;5;241m=\u001b[39m _setup_rechunk(\n\u001b[1;32m    297\u001b[0m     source\u001b[38;5;241m=\u001b[39msource,\n\u001b[1;32m    298\u001b[0m     target_chunks\u001b[38;5;241m=\u001b[39mtarget_chunks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    303\u001b[0m     temp_options\u001b[38;5;241m=\u001b[39mtemp_options,\n\u001b[1;32m    304\u001b[0m )\n\u001b[0;32m--> 305\u001b[0m plan \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_plan\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Rechunked(executor, plan, source, intermediate, target)\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rechunker/executors/dask.py:21\u001b[0m, in \u001b[0;36mDaskExecutor.prepare_plan\u001b[0;34m(self, specs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_plan\u001b[39m(\u001b[38;5;28mself\u001b[39m, specs: Iterable[CopySpec]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Delayed:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_copy_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rechunker/executors/dask.py:96\u001b[0m, in \u001b[0;36m_copy_all\u001b[0;34m(specs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_all\u001b[39m(specs: Iterable[CopySpec],) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Delayed:\n\u001b[0;32m---> 96\u001b[0m     stores_delayed \u001b[38;5;241m=\u001b[39m [_chunked_array_copy(spec) \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m specs]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stores_delayed) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m stores_delayed[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rechunker/executors/dask.py:96\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_all\u001b[39m(specs: Iterable[CopySpec],) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Delayed:\n\u001b[0;32m---> 96\u001b[0m     stores_delayed \u001b[38;5;241m=\u001b[39m [\u001b[43m_chunked_array_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m spec \u001b[38;5;129;01min\u001b[39;00m specs]\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(stores_delayed) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m stores_delayed[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/srv/conda/envs/notebook/lib/python3.9/site-packages/rechunker/executors/dask.py:74\u001b[0m, in \u001b[0;36m_chunked_array_copy\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom-zarr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     73\u001b[0m             root_keys\u001b[38;5;241m.\u001b[39mappend(key)\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(root_keys) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     75\u001b[0m root_key \u001b[38;5;241m=\u001b[39m root_keys[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     77\u001b[0m \u001b[38;5;66;03m# now rewrite the graph\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tqdm(DELIVERY_MODELS) as pbar:\n",
    "    for model in pbar:\n",
    "        for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "            \n",
    "            pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "            pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "            # comment out this block to reproduce rechunked/flipped data on scratch bucket\n",
    "            if fs.exists(pr_spec['output_fp']):\n",
    "                print(f'skipping {model} {scenario} - output already exists')\n",
    "                continue\n",
    "\n",
    "            pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk pr'})\n",
    "            rechunk_data('pr', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "            pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'cap precip'})\n",
    "            with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                cap_precip(pr_input_fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6a681ea-b585-47f7-b70d-11164150b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ee32763-a407-45f6-aa6e-a22dea815fa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13e90cbc82746a281d06afe0cac04f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8b1dd370-3bbc-4b49-9ea4-d83c048aef0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c95905-c518-4f23-88e7-90cf5c5c957c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['pr'].items() for s, fp in v.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aece58-c5e7-405b-8b2e-21d8e27cf16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking_pbar(futures):\n",
    "    status = {'error': 0, 'killed': 0, 'lost': 0}\n",
    "    with tqdm(dd.as_completed(futures), total=len(futures)) as pbar:\n",
    "        for f in pbar:\n",
    "            if f.status in status.keys():\n",
    "                status[f.status] += 1\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "    dd.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c9a6c-c8cc-45fe-a7a7-bbd26f75dc77",
   "metadata": {},
   "source": [
    "# Copy files to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680ecb52-a9f3-4d89-a2d4-a08576c66283",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    pr_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17caf-9ee8-44a5-b65e-1475afd7c7f4",
   "metadata": {},
   "source": [
    "# Deep copy check\n",
    "Check every file against source to ensure a complete copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e772bdc-e317-4586-954e-e93a3dad5991",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    pr_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=True,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c37273-5de3-4bf5-a6ef-46cf6e64defe",
   "metadata": {},
   "source": [
    "### Check pr data in final location\n",
    "Check all pr values, including bounds & NAN checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a438ad-4018-41d7-95cb-a7a0065747db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in tqdm(pr_files):\n",
    "    copy_and_validate(\n",
    "        f,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=True,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=False,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        pbar=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5e885-0bcf-49fd-9df3-fc94d04dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba0ca30-e20e-41c7-9430-aef09efd7deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "outfiles = []\n",
    "for f in (tasmin_files + tasmax_files):\n",
    "    outfiles.append(get_spec_from_input_fp(f)['output_fp'])\n",
    "\n",
    "print(f'outputs are located in the following directory: {os.path.commonpath(outfiles).replace(\"gs:/\", \"gs://\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5651b-8736-4c3e-9d99-62426b7f6e38",
   "metadata": {},
   "source": [
    "To transfer data elsewhere, such as to prep for public delivery or delivery to Catalyst buckets, contact Mike for help with google transfer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dab693-9218-43fa-a790-830c2962fb25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
