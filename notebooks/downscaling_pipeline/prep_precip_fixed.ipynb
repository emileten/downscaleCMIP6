{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "151b3d1c-fbb2-4764-ae15-ab80f4cca63c",
   "metadata": {},
   "source": [
    "# Rechunk, cap, Q/A, and deliver precip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ff668b-94c6-443e-a334-5abd5519bc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = '''\n",
    "v1.1 : initial release (version number set to match temperature). \n",
    "'''.strip()\n",
    "\n",
    "OUTPUT_VERSION = 'v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "024290a8-ef86-42a0-8645-57531ea2229a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.8/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fsspec\n",
    "import requests\n",
    "import contextlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zarr\n",
    "import rechunker\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import dask.distributed as dd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed17d4e5-9951-495a-9da5-7072691c1196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/downscaling\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6adcc35-05c6-4978-b9c3-20ce106d7700",
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIVERY_MODELS = [\n",
    "    'BCC-CSM2-MR',\n",
    "    'FGOALS-g3',\n",
    "    'ACCESS-ESM1-5',\n",
    "    'ACCESS-CM2',\n",
    "    'INM-CM4-8',\n",
    "    'INM-CM5-0',\n",
    "    'MIROC-ES2L',\n",
    "    'MIROC6',\n",
    "    'NorESM2-LM',\n",
    "    'NorESM2-MM',\n",
    "    'GFDL-ESM4',\n",
    "    'GFDL-CM4',\n",
    "    'NESM3',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "120fd3e6-dfbf-4f12-914b-eaee6da083b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTITUTIONS = {    \n",
    "    'BCC-CSM2-MR': 'BCC',\n",
    "    'FGOALS-g3': 'CAS',\n",
    "    'ACCESS-ESM1-5': 'CSIRO',\n",
    "    'ACCESS-CM2': 'CSIRO-ARCCSS',\n",
    "    'INM-CM4-8': 'INM',\n",
    "    'INM-CM5-0': 'INM',\n",
    "    'MIROC-ES2L': 'MIROC',\n",
    "    'MIROC6': 'MIROC',\n",
    "    'NorESM2-LM': 'NCC',\n",
    "    'NorESM2-MM': 'NCC',\n",
    "    'GFDL-ESM4': 'NOAA-GFDL',\n",
    "    'GFDL-CM4': 'NOAA-GFDL',\n",
    "    'NESM3': 'NUIST',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bd7ffa-9fe8-4fb1-8572-26f9f77e6f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_MEMBERS = {\n",
    "    'BCC-CSM2-MR': 'r1i1p1f1',\n",
    "    'FGOALS-g3': 'r1i1p1f1',\n",
    "    'ACCESS-ESM1-5': 'r1i1p1f1',\n",
    "    'ACCESS-CM2': 'r1i1p1f1',\n",
    "    'INM-CM4-8': 'r1i1p1f1',\n",
    "    'INM-CM5-0': 'r1i1p1f1',\n",
    "    'MIROC-ES2L': 'r1i1p1f2',\n",
    "    'MIROC6': 'r1i1p1f1',\n",
    "    'NorESM2-LM': 'r1i1p1f1',\n",
    "    'NorESM2-MM': 'r1i1p1f1',\n",
    "    'GFDL-ESM4': 'r1i1p1f1',\n",
    "    'GFDL-CM4': 'r1i1p1f1',\n",
    "    'NESM3': 'r1i1p1f1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a368f9f-3a8e-4b8e-b1c4-e9cebbdd7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SPECS = {\n",
    "    \"ACCESS-CM2\": \"gn\",\n",
    "    \"MRI-ESM2-0\": \"gn\",\n",
    "    \"CanESM5\": \"gn\",\n",
    "    \"ACCESS-ESM1-5\": \"gn\",\n",
    "    \"MIROC6\": \"gn\",\n",
    "    \"EC-Earth3\": \"gr\",\n",
    "    \"EC-Earth3-Veg-LR\": \"gr\",\n",
    "    \"EC-Earth3-Veg\": \"gr\",\n",
    "    \"MPI-ESM1-2-HR\": \"gn\",\n",
    "    \"CMCC-ESM2\": \"gn\",\n",
    "    \"INM-CM5-0\": \"gr1\",\n",
    "    \"INM-CM4-8\": \"gr1\",\n",
    "    \"MIROC-ES2L\": \"gn\",\n",
    "    \"MPI-ESM1-2-LR\": \"gn\",\n",
    "    \"FGOALS-g3\": \"gn\",\n",
    "    \"BCC-CSM2-MR\": \"gn\",\n",
    "    \"AWI-CM-1-1-MR\": \"gn\",\n",
    "    \"NorESM2-LM\": \"gn\",\n",
    "    \"GFDL-ESM4\": \"gr1\",\n",
    "    \"GFDL-CM4\": \"gr1\",\n",
    "    \"CAMS-CSM1-0\": \"gn\",\n",
    "    \"NorESM2-MM\": \"gn\",\n",
    "    \"NESM3\": \"gn\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00ae1c1-d790-43d5-a023-46ad09eb6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_EXTENSION_SCENARIO = {\n",
    "    \"ACCESS-CM2\": \"ssp370\",\n",
    "    \"MRI-ESM2-0\": \"ssp370\",\n",
    "    \"CanESM5\": \"ssp370\",\n",
    "    \"ACCESS-ESM1-5\": \"ssp370\",\n",
    "    \"MIROC6\": \"ssp370\",\n",
    "    \"EC-Earth3\": \"ssp370\",\n",
    "    \"EC-Earth3-Veg-LR\": \"ssp370\",\n",
    "    \"EC-Earth3-Veg\": \"ssp370\",\n",
    "    \"MPI-ESM1-2-HR\": \"ssp370\",\n",
    "    \"CMCC-ESM2\": \"ssp370\",\n",
    "    \"INM-CM5-0\": \"ssp370\",\n",
    "    \"INM-CM4-8\": \"ssp370\",\n",
    "    \"MIROC-ES2L\": \"ssp370\",\n",
    "    \"MPI-ESM1-2-LR\": \"ssp370\",\n",
    "    \"FGOALS-g3\": \"ssp370\",\n",
    "    \"BCC-CSM2-MR\": \"ssp370\",\n",
    "    \"AWI-CM-1-1-MR\": \"ssp370\",\n",
    "    \"NorESM2-LM\": \"ssp370\",\n",
    "    \"GFDL-ESM4\": \"ssp370\",\n",
    "    \"GFDL-CM4\": \"ssp245\",\n",
    "    \"CAMS-CSM1-0\": \"ssp370\",\n",
    "    \"NorESM2-MM\": \"ssp370\",\n",
    "    \"NESM3\": \"ssp245\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6009ee24-ee82-4727-9402-efae0be145e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEANED_REF_0p25deg_FP = 'gs://support-c23ff1a3/qplad-fine-reference/pr/v20220201000555.zarr'\n",
    "\n",
    "cleaned_gcm_pattern = (\n",
    "    'gs://clean-b1dbca25/cmip6/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{source_version}.zarr'\n",
    ")\n",
    "\n",
    "downscaled_filepatt = (\n",
    "    'gs://downscaled-288ec5ac/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}.zarr'\n",
    ")\n",
    "\n",
    "rechunked_temp_store_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked-temp-store.zarr'\n",
    ")\n",
    "\n",
    "rechunked_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked.zarr'\n",
    ")\n",
    "\n",
    "capped_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-pr-capped.zarr'\n",
    ")\n",
    "\n",
    "OUTPUT_PATTERN = (\n",
    "    'gs://downscaled-288ec5ac/outputs/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{delivery_version}.zarr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80480c37-686e-469e-ad8c-c80c9fda8429",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "823eec05-0206-44b8-93e4-f8f49e9a32ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd1e68e543714b81b8fdeb8c5cd68072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pr:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "precip_spec_file = f'version_specs/precip_{OUTPUT_VERSION}.json'\n",
    "\n",
    "if os.path.isfile(precip_spec_file):\n",
    "    with open(precip_spec_file, 'r') as f:\n",
    "        INPUT_FILE_VERSIONS = json.load(f)\n",
    "\n",
    "else:\n",
    "    pr_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "    for m in tqdm(DELIVERY_MODELS, desc='pr'):\n",
    "\n",
    "        inst = INSTITUTIONS[m]\n",
    "\n",
    "        for act, scen in [\n",
    "            ('CMIP', 'historical'),\n",
    "            ('ScenarioMIP', 'ssp245'),\n",
    "            ('ScenarioMIP', 'ssp370'),\n",
    "        ]:\n",
    "            pr_fps[m][scen] = list(\n",
    "                fs.glob(\n",
    "                    downscaled_filepatt.format(\n",
    "                        activity_id=act,\n",
    "                        institution_id=inst,\n",
    "                        source_id=m,\n",
    "                        experiment_id=scen,\n",
    "                        member_id=ENSEMBLE_MEMBERS[m],\n",
    "                        table_id='day',\n",
    "                        variable_id='pr',\n",
    "                        grid_spec=GRID_SPECS[m],\n",
    "                        run_version='*',\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    pr_max_versions = {\n",
    "        m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "        for m, mspec in pr_fps.items()\n",
    "    }\n",
    "\n",
    "    INPUT_FILE_VERSIONS = {\n",
    "        'version': OUTPUT_VERSION,\n",
    "        'created': pd.Timestamp.now(tz='US/Pacific').strftime('%c'),\n",
    "        'history': HISTORY,\n",
    "        'file_paths': {\n",
    "            'pr': pr_max_versions,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ! mkdir -p version_specs\n",
    "\n",
    "    with open(precip_spec_file, 'w') as f:\n",
    "        f.write(json.dumps(INPUT_FILE_VERSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ce4d86a-7b93-4414-b948-e2917c4ae4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    for v in ['pr']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            raise ValueError(f\"model {m} not found for {v}\")\n",
    "\n",
    "        for s, fp in INPUT_FILE_VERSIONS['file_paths'][v][m].items():\n",
    "            assert m in fp, f\"model name '{m}' not found in filepath '{fp}'\"\n",
    "            assert s in fp, f\"scenario '{s}' not found in filepath '{fp}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "970159e0-3885-4788-8005-deacad19aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'support-data-cc7330d0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CRS_SUPPORT_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a0b37e-8972-4b79-a4f9-ec915ad39b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "CC0_LICENSE_MODELS = ['FGOALS-g3', 'INM-CM4-8', 'INM-CM5-0']\n",
    "\n",
    "CC_BY_LICENSE_MODELS = [\n",
    "    'BCC-CSM2-MR',\n",
    "    'ACCESS-ESM1-5',\n",
    "    'ACCESS-CM2',\n",
    "    'MIROC-ES2L',\n",
    "    'MIROC6',\n",
    "    'NorESM2-LM',\n",
    "    'NorESM2-MM',\n",
    "    'GFDL-CM4',\n",
    "    'GFDL-ESM4',\n",
    "    'NESM3',\n",
    "]\n",
    "\n",
    "for m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS):\n",
    "    for v in ['pr']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            print(m)\n",
    "\n",
    "for m in DELIVERY_MODELS:\n",
    "    assert m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "396955bd-c222-44d3-a42e-e5a071cd464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    hist_extension = HIST_EXTENSION_SCENARIO[m]\n",
    "    if len(INPUT_FILE_VERSIONS['file_paths']['pr'][m]) == 0:\n",
    "        continue\n",
    "\n",
    "    assert hist_extension in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys(), (\n",
    "        f\"{hist_extension} not in {INPUT_FILE_VERSIONS['file_paths']['pr'][m]} for model {m}\"\n",
    "    )\n",
    "\n",
    "    if hist_extension != 'ssp370':\n",
    "        assert 'ssp370' not in INPUT_FILE_VERSIONS['file_paths']['pr'][m].keys()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1947fba-f187-433f-9b56-dfa059c64fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['pr'].items() for s, fp in v.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7b4dd-9494-4c93-859a-dfe3b99380d1",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5029cafa-e4d0-49e8-957f-329a06ba1619",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87996b88-1934-4d80-931d-f486b3f44a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'downscaled_filepatt',\n",
    "    'rechunked_temp_store_pattern',\n",
    "    'rechunked_pattern',\n",
    "    'capped_pattern',\n",
    "    'OUTPUT_PATTERN',\n",
    "])\n",
    "def get_spec_from_input_fp(fp, output_version=OUTPUT_VERSION):\n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        grid,\n",
    "        run_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    spec = dict(\n",
    "        bucket=bucket,\n",
    "        stage=stage,\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        grid=grid,\n",
    "        run_version=run_version,\n",
    "    )\n",
    "\n",
    "    for (name, fpatt) in [\n",
    "        ('downscaled_fp', downscaled_filepatt),\n",
    "        ('rechunk_temp_store_fp', rechunked_temp_store_pattern),\n",
    "        ('rechunked_fp', rechunked_pattern),\n",
    "        ('capped_fp', capped_pattern),\n",
    "        ('output_fp', OUTPUT_PATTERN),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version=run_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "\n",
    "    return spec\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_spec_from_output_fp(fp, output_pattern=OUTPUT_PATTERN):\n",
    "    \n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        output_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    output_fp = output_pattern.format(\n",
    "        activity_id=activity,\n",
    "        institution_id=institution,\n",
    "        source_id=model,\n",
    "        experiment_id=scenario,\n",
    "        member_id=ensemble,\n",
    "        variable_id=variable,\n",
    "        table_id=table,\n",
    "        delivery_version=output_version,\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        output_version=output_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb9d367-2108-49ba-8957-7be42927f6c1",
   "metadata": {},
   "source": [
    "## Stage 1: Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b280a82-83d3-4cd9-bea9-3ed1598447f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'INPUT_FILE_VERSIONS',\n",
    "])\n",
    "def rechunk_data(varname, model, scenario, worker_memory_limit):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=120, cache_timeout=120, requests_timeout=120, read_timeout=120, conn_timeout=120)\n",
    "\n",
    "    target_chunks = {\n",
    "        varname: {'time': 365, 'lat': 360, 'lon': 360},\n",
    "        'time': {'time': 365},\n",
    "        'lat': {'lat': 360},\n",
    "        'lon': {'lon': 360},\n",
    "    }\n",
    "\n",
    "    input_fp = INPUT_FILE_VERSIONS['file_paths'][varname][model][scenario]\n",
    "    input_spec = get_spec_from_input_fp(input_fp)\n",
    "\n",
    "    rechunked_temp_store_fp = input_spec['rechunk_temp_store_fp']\n",
    "    rechunked_fp = input_spec['rechunked_fp']\n",
    "\n",
    "    if fs.isdir(rechunked_fp):\n",
    "        return\n",
    "\n",
    "    mapper = fs.get_mapper(input_fp)\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        rechunked_mapper = fs.get_mapper(rechunked_fp)\n",
    "\n",
    "        chunk_job = rechunker.rechunk(\n",
    "            source=ds,\n",
    "            target_chunks=target_chunks,\n",
    "            max_mem=worker_memory_limit,\n",
    "            target_store=rechunked_mapper,\n",
    "            temp_store=fs.get_mapper(rechunked_temp_store_fp),\n",
    "        )\n",
    "\n",
    "        chunk_job_persist = chunk_job._plan.persist()\n",
    "        dd.wait(chunk_job_persist)\n",
    "\n",
    "    zarr.convenience.consolidate_metadata(rechunked_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c326ce-bb80-463e-a7e7-302571abddd4",
   "metadata": {},
   "source": [
    "## Stage 2: Cap precip at the max(max)*max(max)/max(max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24f74793-15f2-4e28-81f7-5a012ae2cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['cleaned_gcm_pattern', 'HIST_EXTENSION_SCENARIO', 'INPUT_FILE_VERSIONS', 'CLEANED_REF_0p25deg_FP'])\n",
    "def cap_precip(pr_input_fp):\n",
    "\n",
    "    fs = fsspec.filesystem('gs')\n",
    "\n",
    "    pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "    dest_fp = pr_spec['capped_fp']\n",
    "    if fs.isdir(dest_fp):\n",
    "        return\n",
    "\n",
    "    gcm_rechunked = xr.open_zarr(fs.get_mapper(pr_spec['rechunked_fp']))\n",
    "    \n",
    "    source_file_versions = INPUT_FILE_VERSIONS['file_paths']['pr'][pr_spec['model']]\n",
    "\n",
    "    if pr_spec['scenario'] == 'historical':\n",
    "        proj_scen = HIST_EXTENSION_SCENARIO[pr_spec['model']]\n",
    "        proj_fp = get_spec_from_input_fp(source_file_versions[proj_scen])['rechunked_fp']\n",
    "        with xr.open_zarr(fs.get_mapper(proj_fp)) as proj:\n",
    "            source_version_proj = proj.attrs['version_id']\n",
    "\n",
    "        source_version_hist = gcm_rechunked.attrs['version_id']\n",
    "        \n",
    "    else:\n",
    "        proj_scen = pr_spec['scenario']\n",
    "        hist_fp = get_spec_from_input_fp(source_file_versions['historical'])['rechunked_fp']\n",
    "        with xr.open_zarr(fs.get_mapper(hist_fp)) as hist:\n",
    "            source_version_hist = hist.attrs['version_id']\n",
    "\n",
    "        source_version_proj = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    source_version_id = gcm_rechunked.attrs['version_id']\n",
    "\n",
    "    clean_fp_hist = cleaned_gcm_pattern.format(\n",
    "        activity_id='CMIP',\n",
    "        institution_id=pr_spec['institution'],\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id='historical',\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_hist,\n",
    "    )\n",
    "\n",
    "    clean_fp_proj = cleaned_gcm_pattern.format(\n",
    "        activity_id='ScenarioMIP',\n",
    "        institution_id=pr_spec['institution'],\n",
    "        source_id=pr_spec['model'],\n",
    "        experiment_id=proj_scen,\n",
    "        member_id=pr_spec['ensemble'],\n",
    "        table_id=pr_spec['table'],\n",
    "        variable_id=pr_spec['variable'],\n",
    "        grid_spec=pr_spec['grid'],\n",
    "        source_version=source_version_proj,\n",
    "    )\n",
    "\n",
    "    ref_fp = CLEANED_REF_0p25deg_FP\n",
    "\n",
    "    try:\n",
    "        clean_hist = xr.open_zarr(fs.get_mapper(clean_fp_hist))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_hist)\n",
    "\n",
    "    try:\n",
    "        clean_proj = xr.open_zarr(fs.get_mapper(clean_fp_proj))\n",
    "    except zarr.errors.GroupNotFoundError:\n",
    "        raise FileNotFoundError(clean_fp_proj)\n",
    "\n",
    "    ref = xr.open_zarr(fs.get_mapper(ref_fp))\n",
    "\n",
    "    ref_maxpr = ref.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_hist_maxpr = clean_hist.sel(time=slice('1994-12-16', '2015-01-15')).pr.max(dim='time').compute()\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        xr.concat([clean_hist, clean_proj], dim='time')\n",
    "        .pr\n",
    "        .groupby('time.year')\n",
    "        .max(dim='time')\n",
    "        .compute()\n",
    "    )\n",
    "\n",
    "    # convert lons to [-180, 180]\n",
    "\n",
    "    gcm_hist_maxpr = (\n",
    "        gcm_hist_maxpr\n",
    "        .assign_coords(lon=((gcm_hist_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "\n",
    "    gcm_proj_maxpr = (\n",
    "        gcm_proj_maxpr\n",
    "        .assign_coords(lon=((gcm_proj_maxpr.lon % 360 + 180) % 360 - 180))\n",
    "        .sortby('lon')\n",
    "    )\n",
    "    \n",
    "    gcm_proj_maxpr_rolled = (\n",
    "        gcm_proj_maxpr\n",
    "        .rolling(year=21, center=True, min_periods=21).max()\n",
    "    )\n",
    "\n",
    "    gcm_factor = (\n",
    "        (gcm_proj_maxpr_rolled.dropna(dim='year', how='all') / gcm_hist_maxpr)\n",
    "        .rename({'lat': 'lat_coarse', 'lon': 'lon_coarse'})\n",
    "        .sel(lat_coarse=ref_maxpr.lat, lon_coarse=ref_maxpr.lon, method='nearest')\n",
    "        .drop(['lat_coarse', 'lon_coarse'])\n",
    "    )\n",
    "\n",
    "    upper_bound = np.maximum(1, gcm_factor) * ref_maxpr\n",
    "    \n",
    "    with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "        upper_bound_full = (\n",
    "            upper_bound\n",
    "            .reindex(year=np.unique(gcm_rechunked.time.dt.year), method='nearest')\n",
    "            .chunk({'year': 1})\n",
    "            .sel(year=gcm_rechunked.time.dt.year)\n",
    "            .drop('year')\n",
    "        )\n",
    "\n",
    "        gcm_capped = gcm_rechunked.copy(deep=False)\n",
    "\n",
    "        gcm_capped['pr'] = np.minimum(upper_bound_full, gcm_rechunked['pr'])\n",
    "        gcm_capped['pr'].attrs = gcm_rechunked['pr'].attrs\n",
    "        gcm_capped.attrs = gcm_rechunked.attrs\n",
    "\n",
    "        out_mapper = fs.get_mapper(dest_fp)\n",
    "        gcm_capped.to_zarr(out_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d331144c-65e9-49fb-b339-34329b371229",
   "metadata": {},
   "source": [
    "## Stage 3: copy to destination directory & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7820315-4898-48e9-a5ef-19b8c049bb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['CC0_LICENSE_MODELS', 'CC_BY_LICENSE_MODELS'])\n",
    "def quick_check_file(fp, ds, spec):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # check that metadata matches file spec\n",
    "\n",
    "    assert ds.attrs['institution_id'] == spec['institution'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['institution_id']} ≠ {spec['institution']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] == spec['model'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['source_id']} ≠ {spec['model']}\"\n",
    "    )\n",
    "\n",
    "    assert spec['activity'] in ds.attrs['activity_id'], (\n",
    "        f\"invalid attrs in {fp}: {spec['activity']} not in {ds.attrs['activity_id']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['experiment_id'] == spec['scenario'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['experiment_id']} ≠ {spec['scenario']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['variant_label'] == spec['ensemble'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['variant_label']} ≠ {spec['ensemble']}\"\n",
    "    )\n",
    "\n",
    "    if spec['variable'] == 'tasmax':\n",
    "        assert ds['tasmax'].attrs['long_name'] == 'Daily Maximum Near-Surface Air Temperature'\n",
    "        assert ds['tasmax'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'tasmin':\n",
    "        assert ds['tasmin'].attrs['long_name'] == 'Daily Minimum Near-Surface Air Temperature'\n",
    "        assert ds['tasmin'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'pr':\n",
    "#         assert ds['pr'].attrs['long_name'] == \n",
    "        assert ds['pr'].attrs['units'] == 'mm day-1'\n",
    "    else:\n",
    "        raise ValueError(f'variable not recognized: {spec[\"variable\"]}')\n",
    "\n",
    "    # Check licensing fields & endpoint URL\n",
    "\n",
    "    # check that license URL points to a real location and it exists\n",
    "    license_url = ds.attrs['license']\n",
    "    assert ds.attrs['source_id'] in license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {license_url}'\n",
    "    )\n",
    "    r = requests.get(license_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # check that \"Creaive Commons\" and the model name show up on the page\n",
    "    assert ds.attrs['source_id'] in r.text, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    assert \"Creative Commons\" in r.text, (\n",
    "        f'\"Creative Commons\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    # check that \"Creative Commons\" appears in the raw license text\n",
    "\n",
    "    raw_license_url = (\n",
    "        ds.attrs['license']\n",
    "        .replace('github.com', 'raw.githubusercontent.com')\n",
    "        .replace('/blob/', '/')\n",
    "        .replace('/tree/', '/')\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] in raw_license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {raw_license_url}'\n",
    "    )\n",
    "    r = requests.get(raw_license_url)\n",
    "    r.raise_for_status()\n",
    "    assert 'Creative Commons' in r.text, (\n",
    "        f'\"Creative Commons\" not found in license text: {raw_license_url}'\n",
    "    )\n",
    "\n",
    "    if spec['model'] in CC0_LICENSE_MODELS:\n",
    "        assert 'CC0 1.0 Universal' in r.text, (\n",
    "            f\"expected CC0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_LICENSE_MODELS:\n",
    "        assert 'Attribution 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"deploying model with unknown license: {spec['model']} at {fp}\"\n",
    "        )\n",
    "\n",
    "    # Check dimension size & membership\n",
    "\n",
    "    for c in ds.coords.keys():\n",
    "        assert ds.coords[c].notnull().all().item() is True, f\"NaNs found in coordinate '{c}' in {fp}\"\n",
    "\n",
    "    if spec['activity'] == 'ScenarioMIP':\n",
    "        date_range = xr.cftime_range(\"2015-01-01\", \"2099-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "        if len(ds.time) > len(date_range):\n",
    "            date_range = xr.cftime_range(\"2015-01-01\", \"2100-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "    else:\n",
    "        date_range = xr.cftime_range(\"1950-01-01\", \"2014-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "    assert ds.sizes['time'] == len(date_range), (\n",
    "        f\"unexpected length of dimension 'time': length {len(ds.time)}; \"\n",
    "        f\"expected {len(date_range)} in {fp}\"\n",
    "    )\n",
    "\n",
    "    assert date_range.isin(ds.time.dt.floor('D').values).all(), f\"invalid coords in {fp}\"\n",
    "\n",
    "    assert pd.Series(np.arange(-179.875, 180, 0.25)).isin(ds.lon.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "    assert pd.Series(np.arange(-89.875, 90, 0.25)).isin(ds.lat.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "\n",
    "    varnames = list(ds.data_vars.keys())\n",
    "    assert len(varnames) == 1\n",
    "    varname = varnames[0]\n",
    "\n",
    "    assert ds[varname].sizes['lat'] == 720, f\"lat not length 720 in {fp}:\\n{ds}\"\n",
    "    assert ds[varname].sizes['lon'] == 1440, f\"lon not length 1440 in {fp}:\\n{ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "738e7790-3ede-4c5a-86fc-84bd11d42746",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = get_spec_from_input_fp(INPUT_FILE_VERSIONS['file_paths']['pr']['FGOALS-g3']['historical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a313c89-0f35-4ddb-bc6b-975b15dd9dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://downscaled-288ec5ac/outputs/CMIP/CAS/FGOALS-g3/historical/r1i1p1f1/day/pr/v1.1.zarr'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec['output_fp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24ec495f-c345-47eb-beb4-f37821e754f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS'])\n",
    "def validate_outputs(fp, quick=False):\n",
    "    spec = get_spec_from_output_fp(fp)\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=60, cache_timeout=60, requests_timeout=60, read_timeout=60, conn_timeout=60)\n",
    "\n",
    "    mapper = fs.get_mapper(fp)\n",
    "\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        quick_check_file(fp, ds, spec)\n",
    "\n",
    "        if quick:\n",
    "            return\n",
    "\n",
    "        # check variable contents\n",
    "\n",
    "        varnames = list(ds.data_vars.keys())\n",
    "        assert len(varnames) == 1\n",
    "        varname = varnames[0]\n",
    "\n",
    "        to_check = ds[varname].sel(lat=slice(-80, 80))\n",
    "\n",
    "        nans = to_check.isnull().any()\n",
    "        min_val = to_check.min()\n",
    "        max_val = to_check.max()\n",
    "\n",
    "        nans, vmin, vmax = dd.get_client().compute(\n",
    "            [nans, min_val, max_val],\n",
    "            optimize_graph=True,\n",
    "            sync=True,\n",
    "            retries=3,\n",
    "        )\n",
    "\n",
    "        assert nans.item() is False, f\"NaNs found in {fp}\"\n",
    "\n",
    "        if varname == 'tasmax':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'tasmin':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'pr':\n",
    "            allowed_min = 0\n",
    "            allowed_max = 3000\n",
    "        else:\n",
    "            raise ValueError(f'Variable name not recognized: {varname}\\nin file: {fp}')\n",
    "\n",
    "        assert (vmin >= allowed_min).item() is True, (\n",
    "            f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "        assert (vmax <= allowed_max).item() is True, (\n",
    "            f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def copy_and_validate(\n",
    "    source_fp,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    pbar=False,\n",
    "):\n",
    "\n",
    "    spec = get_spec_from_input_fp(source_fp, output_version=output_version)\n",
    "    capped_fp = spec['capped_fp']\n",
    "    output_fp = spec['output_fp']\n",
    "    model = spec['model']\n",
    "    scenario = spec['scenario']\n",
    "\n",
    "    fs = fsspec.filesystem(\n",
    "        'gs',\n",
    "        timeout=360,\n",
    "        cache_timeout=360,\n",
    "        requests_timeout=360, read_timeout=360, conn_timeout=360)\n",
    "\n",
    "    if fs.exists(output_fp):\n",
    "        if overwrite:\n",
    "            fs.remove(output_fp, recursive=True)\n",
    "\n",
    "        else:\n",
    "            if deep_copy_check:\n",
    "                dirs = list([(d, f) for d, dirs, fps in fs.walk(capped_fp) for f in fps])\n",
    "                if pbar:\n",
    "                    dirs = tqdm(dirs)\n",
    "\n",
    "                for d, f in dirs:\n",
    "                    src = capped_fp[:5] + os.path.join(d, f)\n",
    "                    dst = os.path.join(output_fp, os.path.relpath(src, capped_fp))\n",
    "                    assert '..' not in dst\n",
    "                    src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                            break\n",
    "                        except (FileNotFoundError, AssertionError):\n",
    "                            if i == 4:\n",
    "                                raise\n",
    "\n",
    "                            fs.rm(dst)\n",
    "                            fs.copy(src, dst)\n",
    "\n",
    "            if check:\n",
    "                try:\n",
    "                    validate_outputs(\n",
    "                        output_fp,\n",
    "                    )\n",
    "                    return\n",
    "                except (\n",
    "                    AssertionError,\n",
    "                    FileNotFoundError,\n",
    "                    ValueError,\n",
    "                    IOError,\n",
    "                    xr.coding.times.OutOfBoundsDatetime,\n",
    "                    OverflowError,\n",
    "                ):\n",
    "                    if overwrite_on_failure:\n",
    "                        fs.rm(output_fp, recursive=True)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            elif quick_check_and_retry:\n",
    "                try:\n",
    "                    validate_outputs(output_fp, quick=True)\n",
    "                    return\n",
    "                except (\n",
    "                    OverflowError,\n",
    "                    IOError,\n",
    "                    zarr.errors.GroupNotFoundError,\n",
    "                    FileNotFoundError,\n",
    "                    AssertionError,\n",
    "                    ValueError,\n",
    "                ):\n",
    "                    pass\n",
    "\n",
    "                fs.rm(output_fp, recursive=True)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    print(f'copying:\\n\\tsrc:\\t{capped_fp}\\n\\tdst:\\t{output_fp}')\n",
    "    fs.copy(capped_fp, output_fp, recursive=True, batch_size=1000)\n",
    "\n",
    "    if deep_copy_check:\n",
    "        for d, f in list([(d, f) for d, dirs, fps in fs.walk(capped_fp) for f in fps]):\n",
    "            src = capped_fp[:5] + os.path.join(d, f)\n",
    "            dst = os.path.join(output_fp, os.path.relpath(src, capped_fp))\n",
    "            assert '..' not in dst\n",
    "            src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                    break\n",
    "                except (FileNotFoundError, AssertionError):\n",
    "                    if i == 4:\n",
    "                        raise\n",
    "\n",
    "                    fs.rm(dst)\n",
    "                    fs.copy(src, dst)\n",
    "\n",
    "    if check:\n",
    "        validate_outputs(\n",
    "            output_fp,\n",
    "        )\n",
    "    elif quick_check_and_retry:\n",
    "        validate_outputs(output_fp, quick=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f4bb856-3877-4bc3-a99d-2e629bb21011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def kill_cluster_on_error():\n",
    "    try:\n",
    "        yield\n",
    "    except Exception:\n",
    "        # kill the cluster if something unexpected happens during a long-running job\n",
    "        client.restart()\n",
    "        cluster.scale(0)\n",
    "        client.close()\n",
    "        cluster.close()\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a21cb8b0-2f9e-4fee-a9f5-1b569f690ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking_pbar(futures):\n",
    "    status = {'error': 0, 'killed': 0, 'lost': 0}\n",
    "    with tqdm(dd.as_completed(futures), total=len(futures)) as pbar:\n",
    "        for f in pbar:\n",
    "            if f.status in status.keys():\n",
    "                status[f.status] += 1\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "    dd.wait(futures)\n",
    "    for bad_status in status.keys():\n",
    "        if status[bad_status] > 0:\n",
    "            [f for f in futures if f.status == bad_status][0].result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3900e5-bcc7-44c4-ad61-8a840c4ed4f1",
   "metadata": {},
   "source": [
    "# Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9d85359d-5423-4904-a0e0-df79eb486d7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cea52dc75b495b8aa0e913f0fa13bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>GatewayCluster</h2>'), HBox(children=(HTML(value='\\n<div>\\n<style scoped>\\n    …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "client, cluster = rhgk.get_giant_cluster()\n",
    "cluster.scale(60)\n",
    "\n",
    "MAX_MEM = '12GB' # for standard cluster\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0fe81a-cada-4f13-afc6-051c8a6cf9bc",
   "metadata": {},
   "source": [
    "# Prepare final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59af1c52-ae9d-4280-8c37-9232885cbd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c22bf676bd14d60a402abb078e33008",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping BCC-CSM2-MR historical - output already exists\n",
      "skipping BCC-CSM2-MR ssp245 - output already exists\n",
      "skipping BCC-CSM2-MR ssp370 - output already exists\n",
      "skipping BCC-CSM2-MR historical - output already exists\n",
      "skipping BCC-CSM2-MR ssp245 - output already exists\n",
      "skipping BCC-CSM2-MR ssp370 - output already exists\n",
      "skipping FGOALS-g3 historical - output already exists\n",
      "skipping FGOALS-g3 ssp245 - output already exists\n",
      "skipping FGOALS-g3 ssp370 - output already exists\n",
      "skipping FGOALS-g3 historical - output already exists\n",
      "skipping FGOALS-g3 ssp245 - output already exists\n",
      "skipping FGOALS-g3 ssp370 - output already exists\n",
      "skipping ACCESS-ESM1-5 historical - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp245 - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp370 - output already exists\n",
      "skipping ACCESS-ESM1-5 historical - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp245 - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp370 - output already exists\n",
      "skipping ACCESS-CM2 historical - output already exists\n",
      "skipping ACCESS-CM2 ssp245 - output already exists\n",
      "skipping ACCESS-CM2 ssp370 - output already exists\n",
      "skipping ACCESS-CM2 historical - output already exists\n",
      "skipping ACCESS-CM2 ssp245 - output already exists\n",
      "skipping ACCESS-CM2 ssp370 - output already exists\n",
      "skipping INM-CM4-8 historical - output already exists\n",
      "skipping INM-CM4-8 ssp245 - output already exists\n",
      "skipping INM-CM4-8 ssp370 - output already exists\n",
      "skipping INM-CM4-8 historical - output already exists\n",
      "skipping INM-CM4-8 ssp245 - output already exists\n",
      "skipping INM-CM4-8 ssp370 - output already exists\n",
      "skipping INM-CM5-0 historical - output already exists\n",
      "skipping INM-CM5-0 ssp245 - output already exists\n",
      "skipping INM-CM5-0 ssp370 - output already exists\n",
      "skipping INM-CM5-0 historical - output already exists\n",
      "skipping INM-CM5-0 ssp245 - output already exists\n",
      "skipping INM-CM5-0 ssp370 - output already exists\n",
      "skipping MIROC-ES2L historical - output already exists\n",
      "skipping MIROC-ES2L ssp245 - output already exists\n",
      "skipping MIROC-ES2L ssp370 - output already exists\n",
      "skipping MIROC-ES2L historical - output already exists\n",
      "skipping MIROC-ES2L ssp245 - output already exists\n",
      "skipping MIROC-ES2L ssp370 - output already exists\n",
      "skipping MIROC6 historical - output already exists\n",
      "skipping MIROC6 ssp245 - output already exists\n",
      "skipping MIROC6 ssp370 - output already exists\n",
      "skipping MIROC6 historical - output already exists\n",
      "skipping MIROC6 ssp245 - output already exists\n",
      "skipping MIROC6 ssp370 - output already exists\n",
      "skipping NorESM2-LM historical - output already exists\n",
      "skipping NorESM2-LM ssp245 - output already exists\n",
      "skipping NorESM2-LM ssp370 - output already exists\n",
      "skipping NorESM2-LM historical - output already exists\n",
      "skipping NorESM2-LM ssp245 - output already exists\n",
      "skipping NorESM2-LM ssp370 - output already exists\n",
      "skipping NorESM2-MM historical - output already exists\n",
      "skipping NorESM2-MM ssp245 - output already exists\n",
      "skipping NorESM2-MM ssp370 - output already exists\n",
      "skipping NorESM2-MM historical - output already exists\n",
      "skipping NorESM2-MM ssp245 - output already exists\n",
      "skipping NorESM2-MM ssp370 - output already exists\n",
      "skipping GFDL-ESM4 historical - output already exists\n",
      "skipping GFDL-ESM4 ssp245 - output already exists\n",
      "skipping GFDL-ESM4 ssp370 - output already exists\n",
      "skipping GFDL-ESM4 historical - output already exists\n",
      "skipping GFDL-ESM4 ssp245 - output already exists\n",
      "skipping GFDL-ESM4 ssp370 - output already exists\n",
      "skipping GFDL-CM4 historical - output already exists\n",
      "skipping GFDL-CM4 ssp245 - output already exists\n",
      "skipping GFDL-CM4 historical - output already exists\n",
      "skipping GFDL-CM4 ssp245 - output already exists\n",
      "skipping NESM3 historical - output already exists\n",
      "skipping NESM3 ssp245 - output already exists\n",
      "skipping NESM3 historical - output already exists\n",
      "skipping NESM3 ssp245 - output already exists\n"
     ]
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    with tqdm(DELIVERY_MODELS) as pbar:\n",
    "        for model in pbar:\n",
    "            for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "\n",
    "                pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "                pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "                # comment out this block to reproduce rechunked/capped data on scratch bucket\n",
    "                if fs.exists(pr_spec['output_fp']):\n",
    "                    print(f'skipping {model} {scenario} - output already exists')\n",
    "                    continue\n",
    "\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk pr'})\n",
    "                rechunk_data('pr', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "\n",
    "            for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "\n",
    "                pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "                pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "                # comment out this block to reproduce rechunked/capped data on scratch bucket\n",
    "                if fs.exists(pr_spec['output_fp']):\n",
    "                    print(f'skipping {model} {scenario} - output already exists')\n",
    "                    continue\n",
    "\n",
    "                pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'cap precip'})\n",
    "                with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
    "                    cap_precip(pr_input_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c9a6c-c8cc-45fe-a7a7-bbd26f75dc77",
   "metadata": {},
   "source": [
    "# Copy files to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "680ecb52-a9f3-4d89-a2d4-a08576c66283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f9e6bd96d44e53b203c75186cfe673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    pr_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        pr_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        pbar=False,\n",
    "    )\n",
    "\n",
    "    blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a17caf-9ee8-44a5-b65e-1475afd7c7f4",
   "metadata": {},
   "source": [
    "# Deep copy check\n",
    "Check every file against source to ensure a complete copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e772bdc-e317-4586-954e-e93a3dad5991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "164cbe2267ba41b087caf571234e5429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    pr_futures = client.map(\n",
    "        copy_and_validate,\n",
    "        pr_files,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=False,\n",
    "        deep_copy_check=True,\n",
    "        quick_check_and_retry=True,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        pbar=False,\n",
    "    )\n",
    "\n",
    "    blocking_pbar(pr_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c37273-5de3-4bf5-a6ef-46cf6e64defe",
   "metadata": {},
   "source": [
    "### Check pr data in final location\n",
    "Check all pr values, including bounds & NAN checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f9a438ad-4018-41d7-95cb-a7a0065747db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf5f20357724463c98471ff0b7765a5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with kill_cluster_on_error():\n",
    "    for f in tqdm(pr_files):\n",
    "        copy_and_validate(\n",
    "            f,\n",
    "            output_version=OUTPUT_VERSION,\n",
    "            check=True,\n",
    "            deep_copy_check=False,\n",
    "            quick_check_and_retry=False,\n",
    "            overwrite=False,\n",
    "            overwrite_on_failure=False,\n",
    "            pbar=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bb5e885-0bcf-49fd-9df3-fc94d04dfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ba0ca30-e20e-41c7-9430-aef09efd7deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs are located in the following directory: gs://downscaled-288ec5ac/outputs\n"
     ]
    }
   ],
   "source": [
    "outfiles = []\n",
    "for f in (pr_files):\n",
    "    outfiles.append(get_spec_from_input_fp(f)['output_fp'])\n",
    "\n",
    "print(f'outputs are located in the following directory: {os.path.commonpath(outfiles).replace(\"gs:/\", \"gs://\").replace(\":///\", \"://\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5651b-8736-4c3e-9d99-62426b7f6e38",
   "metadata": {},
   "source": [
    "To transfer data elsewhere, such as to prep for public delivery or delivery to Catalyst buckets, contact Mike for help with google transfer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97dab693-9218-43fa-a790-830c2962fb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff6d456a6d140b996004853a6988eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://downscaled-288ec5ac/outputs/CMIP/BCC/BCC-CSM2-MR/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/BCC/BCC-CSM2-MR/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/BCC/BCC-CSM2-MR/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/CAS/FGOALS-g3/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CAS/FGOALS-g3/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CAS/FGOALS-g3/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/CSIRO/ACCESS-ESM1-5/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CSIRO/ACCESS-ESM1-5/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CSIRO/ACCESS-ESM1-5/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/CSIRO-ARCCSS/ACCESS-CM2/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/CSIRO-ARCCSS/ACCESS-CM2/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/INM/INM-CM4-8/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/INM/INM-CM4-8/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/INM/INM-CM4-8/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/INM/INM-CM5-0/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/INM/INM-CM5-0/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/INM/INM-CM5-0/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/MIROC/MIROC-ES2L/historical/r1i1p1f2/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/MIROC/MIROC-ES2L/ssp245/r1i1p1f2/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/MIROC/MIROC-ES2L/ssp370/r1i1p1f2/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/MIROC/MIROC6/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/MIROC/MIROC6/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/MIROC/MIROC6/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/NCC/NorESM2-LM/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NCC/NorESM2-LM/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NCC/NorESM2-LM/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/NCC/NorESM2-MM/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NCC/NorESM2-MM/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NCC/NorESM2-MM/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/NOAA-GFDL/GFDL-ESM4/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NOAA-GFDL/GFDL-ESM4/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NOAA-GFDL/GFDL-ESM4/ssp370/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/NOAA-GFDL/GFDL-CM4/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NOAA-GFDL/GFDL-CM4/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/CMIP/NUIST/NESM3/historical/r1i1p1f1/day/pr/v1.1.zarr\n",
      "gs://downscaled-288ec5ac/outputs/ScenarioMIP/NUIST/NESM3/ssp245/r1i1p1f1/day/pr/v1.1.zarr\n"
     ]
    }
   ],
   "source": [
    "with tqdm(DELIVERY_MODELS) as pbar:\n",
    "    for model in pbar:\n",
    "        for scenario in INPUT_FILE_VERSIONS['file_paths']['pr'][model].keys():\n",
    "\n",
    "            pr_input_fp = INPUT_FILE_VERSIONS['file_paths']['pr'][model][scenario]\n",
    "            pr_spec = get_spec_from_input_fp(pr_input_fp)\n",
    "\n",
    "            print(pr_spec['output_fp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2911590d-af25-472e-a781-d19363a44b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
