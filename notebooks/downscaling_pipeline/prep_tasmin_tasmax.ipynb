{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Rechunk, flip, Q/A, and deliver tasmin & tasmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = '''\n",
    "v1.1 : switch to additive QDM tasmin with swapping where tasmin > tasmax; also include\n",
    "       regridding with nearest neighbor patch in QPLAD.\n",
    "v1.0 : initial release; QDM tasmax (additive) and DTR (multiplicative)\n",
    "'''.strip()\n",
    "\n",
    "OUTPUT_VERSION = 'v1.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/dask_gateway/client.py:21: FutureWarning: format_bytes is deprecated and will be removed in a future release. Please use dask.utils.format_bytes instead.\n",
      "  from distributed.utils import LoopRunner, format_bytes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import fsspec\n",
    "import requests\n",
    "import contextlib\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import zarr\n",
    "import rechunker\n",
    "import dask\n",
    "import rhg_compute_tools.kubernetes as rhgk\n",
    "import rhg_compute_tools.utils as rhgu\n",
    "import dask.distributed as dd\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/repositories/downscaleCMIP6/notebooks/downscaling_pipeline\n"
     ]
    }
   ],
   "source": [
    "! pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "DELIVERY_MODELS = [\n",
    "    # 'BCC-CSM2-MR',\n",
    "    # 'FGOALS-g3',\n",
    "    # 'ACCESS-ESM1-5',\n",
    "    # 'ACCESS-CM2',\n",
    "    # 'INM-CM4-8',\n",
    "    # 'INM-CM5-0',\n",
    "    # 'MIROC-ES2L',\n",
    "    # 'MIROC6',\n",
    "    # 'NorESM2-LM',\n",
    "    # 'NorESM2-MM',\n",
    "    # 'GFDL-ESM4',\n",
    "    # 'GFDL-CM4',\n",
    "    'NESM3',\n",
    "    #'CMCC-CM2-SR5',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTITUTIONS = {    \n",
    "    'BCC-CSM2-MR': 'BCC',\n",
    "    'FGOALS-g3': 'CAS',\n",
    "    'ACCESS-ESM1-5': 'CSIRO',\n",
    "    'ACCESS-CM2': 'CSIRO-ARCCSS',\n",
    "    'INM-CM4-8': 'INM',\n",
    "    'INM-CM5-0': 'INM',\n",
    "    'MIROC-ES2L': 'MIROC',\n",
    "    'MIROC6': 'MIROC',\n",
    "    'NorESM2-LM': 'NCC',\n",
    "    'NorESM2-MM': 'NCC',\n",
    "    'GFDL-ESM4': 'NOAA-GFDL',\n",
    "    'GFDL-CM4': 'NOAA-GFDL',\n",
    "    'NESM3': 'NUIST',\n",
    "    'CMCC-CM2-SR5':'CMCC',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENSEMBLE_MEMBERS = {\n",
    "    'BCC-CSM2-MR': 'r1i1p1f1',\n",
    "    'FGOALS-g3': 'r1i1p1f1',\n",
    "    'ACCESS-ESM1-5': 'r1i1p1f1',\n",
    "    'ACCESS-CM2': 'r1i1p1f1',\n",
    "    'INM-CM4-8': 'r1i1p1f1',\n",
    "    'INM-CM5-0': 'r1i1p1f1',\n",
    "    'MIROC-ES2L': 'r1i1p1f2',\n",
    "    'MIROC6': 'r1i1p1f1',\n",
    "    'NorESM2-LM': 'r1i1p1f1',\n",
    "    'NorESM2-MM': 'r1i1p1f1',\n",
    "    'GFDL-ESM4': 'r1i1p1f1',\n",
    "    'GFDL-CM4': 'r1i1p1f1',\n",
    "    'NESM3': 'r1i1p1f1',\n",
    "    'CMCC-CM2-SR5':'r1i1p1f1',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SPECS = {\n",
    "    \"ACCESS-CM2\": \"gn\",\n",
    "    \"MRI-ESM2-0\": \"gn\",\n",
    "    \"CanESM5\": \"gn\",\n",
    "    \"ACCESS-ESM1-5\": \"gn\",\n",
    "    \"MIROC6\": \"gn\",\n",
    "    \"EC-Earth3\": \"gr\",\n",
    "    \"EC-Earth3-Veg-LR\": \"gr\",\n",
    "    \"EC-Earth3-Veg\": \"gr\",\n",
    "    \"MPI-ESM1-2-HR\": \"gn\",\n",
    "    \"CMCC-ESM2\": \"gn\",\n",
    "    \"INM-CM5-0\": \"gr1\",\n",
    "    \"INM-CM4-8\": \"gr1\",\n",
    "    \"MIROC-ES2L\": \"gn\",\n",
    "    \"MPI-ESM1-2-LR\": \"gn\",\n",
    "    \"FGOALS-g3\": \"gn\",\n",
    "    \"BCC-CSM2-MR\": \"gn\",\n",
    "    \"AWI-CM-1-1-MR\": \"gn\",\n",
    "    \"NorESM2-LM\": \"gn\",\n",
    "    \"GFDL-ESM4\": \"gr1\",\n",
    "    \"GFDL-CM4\": \"gr1\",\n",
    "    \"CAMS-CSM1-0\": \"gn\",\n",
    "    \"NorESM2-MM\": \"gn\",\n",
    "    \"NESM3\": \"gn\",\n",
    "    'CMCC-CM2-SR5':'gn',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_filepatt = (\n",
    "    'gs://downscaled-288ec5ac/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}.zarr'\n",
    ")\n",
    "\n",
    "rechunked_temp_store_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked-temp-store.zarr'\n",
    ")\n",
    "\n",
    "rechunked_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-rechunked.zarr'\n",
    ")\n",
    "\n",
    "fipped_pattern = (\n",
    "    'gs://scratch-170cd6ec/stage/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{grid_spec}/'\n",
    "    '{run_version}-tasminmax-flipped.zarr'\n",
    ")\n",
    "\n",
    "OUTPUT_PATTERN = (\n",
    "    'gs://downscaled-288ec5ac/outputs/{activity_id}/{institution_id}/{source_id}/'\n",
    "    '{experiment_id}/{member_id}/{table_id}/{variable_id}/{delivery_version}.zarr'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.filesystem('gs', timeout=360, cache_timeout=360, requests_timeout=360, read_timeout=360, conn_timeout=360, token='/opt/gcsfuse_tokens/impactlab-data.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_file = f'version_specs/{OUTPUT_VERSION}.json'\n",
    "\n",
    "if os.path.isfile(spec_file):\n",
    "    with open(spec_file, 'r') as f:\n",
    "        INPUT_FILE_VERSIONS = json.load(f)\n",
    "\n",
    "else:\n",
    "    tasmax_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "    for m in tqdm(DELIVERY_MODELS, desc='tasmax'):\n",
    "\n",
    "        inst = INSTITUTIONS[m]\n",
    "\n",
    "        for act, scen in [\n",
    "            ('CMIP', 'historical'),\n",
    "            ('ScenarioMIP', 'ssp245'),\n",
    "            ('ScenarioMIP', 'ssp370'),\n",
    "        ]:\n",
    "            tasmax_fps[m][scen] = list(\n",
    "                fs.glob(\n",
    "                    downscaled_filepatt.format(\n",
    "                        activity_id=act,\n",
    "                        institution_id=inst,\n",
    "                        source_id=m,\n",
    "                        experiment_id=scen,\n",
    "                        member_id=ENSEMBLE_MEMBERS[m],\n",
    "                        table_id='day',\n",
    "                        variable_id='tasmax',\n",
    "                        grid_spec=GRID_SPECS[m],\n",
    "                        run_version='*',\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    tasmax_max_versions = {\n",
    "        m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "        for m, mspec in tasmax_fps.items()\n",
    "    }\n",
    "\n",
    "    tasmin_fps = {m: {} for m in DELIVERY_MODELS}\n",
    "\n",
    "    for m in tqdm(DELIVERY_MODELS, desc='tasmin'):\n",
    "\n",
    "        inst = INSTITUTIONS[m]\n",
    "\n",
    "        for act, scen in [\n",
    "            ('CMIP', 'historical'),\n",
    "            ('ScenarioMIP', 'ssp245'),\n",
    "            ('ScenarioMIP', 'ssp370'),\n",
    "        ]:\n",
    "            tasmin_fps[m][scen] = list(\n",
    "                fs.glob(\n",
    "                    downscaled_filepatt.format(\n",
    "                        activity_id=act,\n",
    "                        institution_id=inst,\n",
    "                        source_id=m,\n",
    "                        experiment_id=scen,\n",
    "                        member_id=ENSEMBLE_MEMBERS[m],\n",
    "                        table_id='day',\n",
    "                        variable_id='tasmin',\n",
    "                        grid_spec=GRID_SPECS[m],\n",
    "                        run_version='*',\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    tasmin_max_versions = {\n",
    "        m: {s: max(vs) for s, vs in mspec.items() if len(vs) > 0}\n",
    "        for m, mspec in tasmin_fps.items()\n",
    "    }\n",
    "\n",
    "    INPUT_FILE_VERSIONS = {\n",
    "        'version': OUTPUT_VERSION,\n",
    "        'created': pd.Timestamp.now(tz='US/Pacific').strftime('%c'),\n",
    "        'history': HISTORY,\n",
    "        'file_paths': {\n",
    "            'tasmin': tasmin_max_versions,\n",
    "            'tasmax': tasmax_max_versions,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    ! mkdir -p version_specs\n",
    "\n",
    "    with open(spec_file, 'w') as f:\n",
    "        f.write(json.dumps(INPUT_FILE_VERSIONS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in DELIVERY_MODELS:\n",
    "    for v in ['tasmin', 'tasmax']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            raise ValueError(f\"model {m} not found for {v}\")\n",
    "\n",
    "        for s, fp in INPUT_FILE_VERSIONS['file_paths'][v][m].items():\n",
    "            assert m in fp, f\"model name '{m}' not found in filepath '{fp}'\"\n",
    "            assert s in fp, f\"scenario '{s}' not found in filepath '{fp}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGOALS-g3\n",
      "FGOALS-g3\n",
      "INM-CM4-8\n",
      "INM-CM4-8\n",
      "INM-CM5-0\n",
      "INM-CM5-0\n",
      "BCC-CSM2-MR\n",
      "BCC-CSM2-MR\n",
      "ACCESS-ESM1-5\n",
      "ACCESS-ESM1-5\n",
      "ACCESS-CM2\n",
      "ACCESS-CM2\n",
      "MIROC-ES2L\n",
      "MIROC-ES2L\n",
      "MIROC6\n",
      "MIROC6\n",
      "NorESM2-LM\n",
      "NorESM2-LM\n",
      "NorESM2-MM\n",
      "NorESM2-MM\n",
      "GFDL-CM4\n",
      "GFDL-CM4\n",
      "GFDL-ESM4\n",
      "GFDL-ESM4\n",
      "NESM3\n",
      "NESM3\n"
     ]
    }
   ],
   "source": [
    "CC0_LICENSE_MODELS = ['FGOALS-g3', 'INM-CM4-8', 'INM-CM5-0']\n",
    "\n",
    "CC_BY_LICENSE_MODELS = [\n",
    "    'BCC-CSM2-MR',\n",
    "    'ACCESS-ESM1-5',\n",
    "    'ACCESS-CM2',\n",
    "    'MIROC-ES2L',\n",
    "    'MIROC6',\n",
    "    'NorESM2-LM',\n",
    "    'NorESM2-MM',\n",
    "    'GFDL-CM4',\n",
    "    'GFDL-ESM4',\n",
    "    'NESM3',\n",
    "    'CMCC-CM2-SR5',\n",
    "]\n",
    "\n",
    "for m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS):\n",
    "    for v in ['tasmin', 'tasmax']:\n",
    "        if m not in INPUT_FILE_VERSIONS['file_paths'][v]:\n",
    "            print(m)\n",
    "\n",
    "for m in DELIVERY_MODELS:\n",
    "    assert m in (CC0_LICENSE_MODELS + CC_BY_LICENSE_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'downscaled_filepatt',\n",
    "    'rechunked_temp_store_pattern',\n",
    "    'rechunked_pattern',\n",
    "    'fipped_pattern',\n",
    "    'OUTPUT_PATTERN',\n",
    "])\n",
    "def get_spec_from_input_fp(fp, output_version=OUTPUT_VERSION):\n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        grid,\n",
    "        run_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    spec = dict(\n",
    "        bucket=bucket,\n",
    "        stage=stage,\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        grid=grid,\n",
    "        run_version=run_version,\n",
    "    )\n",
    "\n",
    "    for (name, fpatt) in [\n",
    "        ('downscaled_fp', downscaled_filepatt),\n",
    "        ('rechunk_temp_store_fp', rechunked_temp_store_pattern),\n",
    "        ('rechunked_fp', rechunked_pattern),\n",
    "        ('flipped_fp', fipped_pattern),\n",
    "        ('output_fp', OUTPUT_PATTERN),\n",
    "    ]:\n",
    "        spec[name] = fpatt.format(\n",
    "            activity_id=activity,\n",
    "            institution_id=institution,\n",
    "            source_id=model,\n",
    "            experiment_id=scenario,\n",
    "            member_id=ensemble,\n",
    "            variable_id=variable,\n",
    "            table_id=table,\n",
    "            grid_spec=grid,\n",
    "            run_version=run_version,\n",
    "            delivery_version=output_version,\n",
    "        )\n",
    "\n",
    "    return spec\n",
    "\n",
    "@rhgu.block_globals\n",
    "def get_spec_from_output_fp(fp, output_pattern=OUTPUT_PATTERN):\n",
    "    \n",
    "    (\n",
    "        bucket,\n",
    "        stage,\n",
    "        activity,\n",
    "        institution,\n",
    "        model,\n",
    "        scenario,\n",
    "        ensemble,\n",
    "        table,\n",
    "        variable,\n",
    "        output_version,\n",
    "    ) = os.path.splitext(fp)[0].replace('gs://', '').split('/')\n",
    "\n",
    "    output_fp = output_pattern.format(\n",
    "        activity_id=activity,\n",
    "        institution_id=institution,\n",
    "        source_id=model,\n",
    "        experiment_id=scenario,\n",
    "        member_id=ensemble,\n",
    "        variable_id=variable,\n",
    "        table_id=table,\n",
    "        delivery_version=output_version,\n",
    "    )\n",
    "\n",
    "    return dict(\n",
    "        activity=activity,\n",
    "        institution=institution,\n",
    "        model=model,\n",
    "        scenario=scenario,\n",
    "        ensemble=ensemble,\n",
    "        table=table,\n",
    "        variable=variable,\n",
    "        output_version=output_version,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Rechunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=[\n",
    "    'INPUT_FILE_VERSIONS',\n",
    "])\n",
    "def rechunk_data(varname, model, scenario, worker_memory_limit):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=360, cache_timeout=360, requests_timeout=360, read_timeout=360, conn_timeout=360, token='/opt/gcsfuse_tokens/impactlab-data.json')\n",
    "\n",
    "    target_chunks = {\n",
    "        varname: {'time': 365, 'lat': 360, 'lon': 360},\n",
    "        'time': {'time': 365},\n",
    "        'lat': {'lat': 360},\n",
    "        'lon': {'lon': 360},\n",
    "    }\n",
    "\n",
    "    input_fp = INPUT_FILE_VERSIONS['file_paths'][varname][model][scenario]\n",
    "    input_spec = get_spec_from_input_fp(input_fp)\n",
    "\n",
    "    rechunked_temp_store_fp = input_spec['rechunk_temp_store_fp']\n",
    "    rechunked_fp = input_spec['rechunked_fp']\n",
    "\n",
    "    if fs.isdir(rechunked_fp):\n",
    "        return\n",
    "\n",
    "    mapper = fs.get_mapper(input_fp)\n",
    "    with xr.open_zarr(mapper) as ds:\n",
    "\n",
    "        rechunked_mapper = fs.get_mapper(rechunked_fp)\n",
    "\n",
    "        chunk_job = rechunker.rechunk(\n",
    "            source=ds,\n",
    "            target_chunks=target_chunks,\n",
    "            max_mem=worker_memory_limit,\n",
    "            target_store=rechunked_mapper,\n",
    "            temp_store=fs.get_mapper(rechunked_temp_store_fp),\n",
    "        )\n",
    "\n",
    "        chunk_job_persist = chunk_job._plan.persist()\n",
    "        dd.wait(chunk_job_persist)\n",
    "\n",
    "    zarr.convenience.consolidate_metadata(rechunked_mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Flip Negative DTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS', 'rechunked_pattern', 'fipped_pattern'])\n",
    "def flip_negative_dtr(model, scenario):\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=360, cache_timeout=360, requests_timeout=360, read_timeout=360, conn_timeout=360, token='/opt/gcsfuse_tokens/impactlab-data.json')\n",
    "    client = dd.get_client()\n",
    "\n",
    "    tasmin_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmin'][model][scenario]\n",
    "    tasmin_input_spec = get_spec_from_input_fp(tasmin_input_fp)\n",
    "    tasmin_rechunked_fp = tasmin_input_spec['rechunked_fp']\n",
    "    tasmin_fipped_fp = tasmin_input_spec['flipped_fp']\n",
    "\n",
    "    tasmax_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmax'][model][scenario]\n",
    "    tasmax_input_spec = get_spec_from_input_fp(tasmax_input_fp)\n",
    "    tasmax_rechunked_fp = tasmax_input_spec['rechunked_fp']\n",
    "    tasmax_fipped_fp = tasmax_input_spec['flipped_fp']\n",
    "\n",
    "    if fs.isdir(tasmin_fipped_fp) and fs.isdir(tasmax_fipped_fp):\n",
    "        return\n",
    "\n",
    "    tasmin_in_mapper = fs.get_mapper(tasmin_rechunked_fp)\n",
    "    tasmax_in_mapper = fs.get_mapper(tasmax_rechunked_fp)\n",
    "\n",
    "    with xr.open_zarr(tasmin_in_mapper) as tasmin_ds, xr.open_zarr(tasmax_in_mapper) as tasmax_ds:\n",
    "\n",
    "        tasmin_ds_out = tasmin_ds.copy(deep=False)\n",
    "        tasmax_ds_out = tasmax_ds.copy(deep=False)\n",
    "\n",
    "        tasmin_ds_out['tasmin'] = np.minimum(tasmin_ds['tasmin'], tasmax_ds['tasmax'])\n",
    "        tasmax_ds_out['tasmax'] = np.maximum(tasmin_ds['tasmin'], tasmax_ds['tasmax'])\n",
    "\n",
    "        tasmin_ds_out['tasmin'].attrs.update(tasmin_ds['tasmin'].attrs)\n",
    "        tasmax_ds_out['tasmax'].attrs.update(tasmax_ds['tasmax'].attrs)\n",
    "\n",
    "        dtr = (tasmax_ds_out['tasmax'] - tasmin_ds_out['tasmin'])\n",
    "        min_dtr = dtr.min()\n",
    "        dtr_usually_positive = (dtr > 0.1).mean()\n",
    "\n",
    "        tasmin_out_mapper = fs.get_mapper(tasmin_fipped_fp)\n",
    "        tasmax_out_mapper = fs.get_mapper(tasmax_fipped_fp)\n",
    "\n",
    "        write_tasmin = tasmin_ds_out.to_zarr(tasmin_out_mapper, consolidated=True, compute=False)\n",
    "        write_tasmax = tasmax_ds_out.to_zarr(tasmax_out_mapper, consolidated=True, compute=False)\n",
    "\n",
    "        min_dtr, dtr_usually_positive, write_tasmin, write_tasmax = client.compute(\n",
    "            [min_dtr, dtr_usually_positive, write_tasmin, write_tasmax],\n",
    "            optimize_graph=True,\n",
    "            retries=3,\n",
    "            sync=True,\n",
    "        )\n",
    "\n",
    "        assert (min_dtr >= 0).item() is True, (\n",
    "            f\"DTR not always positive after flip: min DTR: {min_dtr.item()}\"\n",
    "        )\n",
    "\n",
    "        assert (dtr_usually_positive > 0.99).item() is True, (\n",
    "            f\"DTR not almost always > 0.1: {dtr_usually_positive.item()}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: copy to destination directory & validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['CC0_LICENSE_MODELS', 'CC_BY_LICENSE_MODELS'])\n",
    "def quick_check_file(fp, ds, spec):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    # check that metadata matches file spec\n",
    "\n",
    "    assert ds.attrs['institution_id'] == spec['institution'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['institution_id']} ≠ {spec['institution']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] == spec['model'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['source_id']} ≠ {spec['model']}\"\n",
    "    )\n",
    "\n",
    "    assert spec['activity'] in ds.attrs['activity_id'], (\n",
    "        f\"invalid attrs in {fp}: {spec['activity']} not in {ds.attrs['activity_id']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['experiment_id'] == spec['scenario'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['experiment_id']} ≠ {spec['scenario']}\"\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['variant_label'] == spec['ensemble'], (\n",
    "        f\"invalid attrs in {fp}: {ds.attrs['variant_label']} ≠ {spec['ensemble']}\"\n",
    "    )\n",
    "\n",
    "    if spec['variable'] == 'tasmax':\n",
    "        assert ds['tasmax'].attrs['long_name'] == 'Daily Maximum Near-Surface Air Temperature'\n",
    "        assert ds['tasmax'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'tasmin':\n",
    "        assert ds['tasmin'].attrs['long_name'] == 'Daily Minimum Near-Surface Air Temperature'\n",
    "        assert ds['tasmin'].attrs['units'] == 'K'\n",
    "    elif spec['variable'] == 'pr':\n",
    "        raise NotImplementedError()\n",
    "#         assert ds['tasmax'].attrs['units'] == 'mm/day'\n",
    "    else:\n",
    "        raise ValueError(f'variable not recognized: {spec[\"variable\"]}')\n",
    "\n",
    "    # Check licensing fields & endpoint URL\n",
    "\n",
    "    # check that license URL points to a real location and it exists\n",
    "    license_url = ds.attrs['license']\n",
    "    assert ds.attrs['source_id'] in license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {license_url}'\n",
    "    )\n",
    "    r = requests.get(license_url)\n",
    "    r.raise_for_status()\n",
    "\n",
    "    # check that \"Creaive Commons\" and the model name show up on the page\n",
    "    assert ds.attrs['source_id'] in r.text, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    assert \"Creative Commons\" in r.text, (\n",
    "        f'\"Creative Commons\" not found on license page: {license_url}'\n",
    "    )\n",
    "\n",
    "    # check that \"Creative Commons\" appears in the raw license text\n",
    "\n",
    "    raw_license_url = (\n",
    "        ds.attrs['license']\n",
    "        .replace('github.com', 'raw.githubusercontent.com')\n",
    "        .replace('/blob/', '/')\n",
    "        .replace('/tree/', '/')\n",
    "    )\n",
    "\n",
    "    assert ds.attrs['source_id'] in raw_license_url, (\n",
    "        f'model \"{ds.attrs[\"source_id\"]}\" not found in license url: {raw_license_url}'\n",
    "    )\n",
    "    r = requests.get(raw_license_url)\n",
    "    r.raise_for_status()\n",
    "    assert 'Creative Commons' in r.text, (\n",
    "        f'\"Creative Commons\" not found in license text: {raw_license_url}'\n",
    "    )\n",
    "\n",
    "    if spec['model'] in CC0_LICENSE_MODELS:\n",
    "        assert 'CC0 1.0 Universal' in r.text, (\n",
    "            f\"expected CC0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    elif spec['model'] in CC_BY_LICENSE_MODELS:\n",
    "        assert 'Attribution 4.0 International' in r.text, (\n",
    "            f\"expected CC-BY 4.0 license for {spec['model']} at {fp}\"\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"deploying model with unknown license: {spec['model']} at {fp}\"\n",
    "        )\n",
    "\n",
    "    # Check dimension size & membership\n",
    "\n",
    "    for c in ds.coords.keys():\n",
    "        assert ds.coords[c].notnull().all().item() is True, f\"NaNs found in coordinate '{c}' in {fp}\"\n",
    "\n",
    "    if spec['activity'] == 'ScenarioMIP':\n",
    "        date_range = xr.cftime_range(\"2015-01-01\", \"2099-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "        if len(ds.time) > len(date_range):\n",
    "            date_range = xr.cftime_range(\"2015-01-01\", \"2100-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "    else:\n",
    "        date_range = xr.cftime_range(\"1950-01-01\", \"2014-12-31\", freq=\"D\", calendar=\"noleap\")\n",
    "\n",
    "    assert ds.sizes['time'] == len(date_range), (\n",
    "        f\"unexpected length of dimension 'time': length {len(ds.time)}; \"\n",
    "        f\"expected {len(date_range)} in {fp}\"\n",
    "    )\n",
    "\n",
    "    assert date_range.isin(ds.time.dt.floor('D').values).all(), f\"invalid coords in {fp}\"\n",
    "\n",
    "    assert pd.Series(np.arange(-179.875, 180, 0.25)).isin(ds.lon.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "    assert pd.Series(np.arange(-89.875, 90, 0.25)).isin(ds.lat.values).all(), (\n",
    "        f\"invalid coords in {fp}\"\n",
    "    )\n",
    "\n",
    "    varnames = list(ds.data_vars.keys())\n",
    "    assert len(varnames) == 1\n",
    "    varname = varnames[0]\n",
    "\n",
    "    assert ds[varname].sizes['lat'] == 720, f\"lat not length 720 in {fp}:\\n{ds}\"\n",
    "    assert ds[varname].sizes['lon'] == 1440, f\"lon not length 1440 in {fp}:\\n{ds}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = get_spec_from_input_fp(INPUT_FILE_VERSIONS['file_paths']['tasmax']['CMCC-CM2-SR5']['historical'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://downscaled-288ec5ac/outputs/CMIP/CMCC/CMCC-CM2-SR5/historical/r1i1p1f1/day/tasmax/v1.1.zarr'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec['output_fp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@rhgu.block_globals(whitelist=['INPUT_FILE_VERSIONS'])\n",
    "def validate_outputs(fp, quick=False, check_dtr=False):\n",
    "    spec = get_spec_from_output_fp(fp)\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=360, cache_timeout=360, requests_timeout=360, read_timeout=360, conn_timeout=360, token='/opt/gcsfuse_tokens/impactlab-data.json')\n",
    "\n",
    "    mapper = fs.get_mapper(fp)\n",
    "\n",
    "    if check_dtr and (spec['variable'] != 'tasmin'):\n",
    "        raise ValueError('check_dtr can only be used with variable == \"tasmin\"')\n",
    "\n",
    "    if check_dtr:\n",
    "        tasmax_spec = get_spec_from_input_fp(\n",
    "            INPUT_FILE_VERSIONS['file_paths']['tasmax'][spec['model']][spec['scenario']]\n",
    "        )\n",
    "\n",
    "        tasmax_fp = tasmax_spec['output_fp']\n",
    "        tasmax_mapper = fs.get_mapper(tasmax_fp)\n",
    "        tasmax_opener = xr.open_zarr(tasmax_mapper)\n",
    "\n",
    "    else:\n",
    "        tasmax_opener = contextlib.nullcontext()\n",
    "\n",
    "    with xr.open_zarr(mapper) as ds, tasmax_opener as tasmax_ds:\n",
    "\n",
    "        quick_check_file(fp, ds, spec)\n",
    "\n",
    "        if quick:\n",
    "            return\n",
    "\n",
    "        # check variable contents\n",
    "\n",
    "        varnames = list(ds.data_vars.keys())\n",
    "        assert len(varnames) == 1\n",
    "        varname = varnames[0]\n",
    "\n",
    "        to_check = ds[varname].sel(lat=slice(-80, 80))\n",
    "\n",
    "        nans = to_check.isnull().any()\n",
    "        min_val = to_check.min()\n",
    "        max_val = to_check.max()\n",
    "\n",
    "        if check_dtr:\n",
    "            assert varname == 'tasmin'\n",
    "            min_dtr = (tasmax_ds.tasmax.sel(lat=slice(-80, 80)) - to_check).min()\n",
    "        else:\n",
    "            min_dtr = dask.delayed(lambda: None)()\n",
    "\n",
    "        nans, vmin, vmax, min_dtr = dd.get_client().compute(\n",
    "            [nans, min_val, max_val, min_dtr],\n",
    "            optimize_graph=True,\n",
    "            sync=True,\n",
    "            retries=3,\n",
    "        )\n",
    "\n",
    "        assert nans.item() is False, f\"NaNs found in {fp}\"\n",
    "\n",
    "        if varname == 'tasmax':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'tasmin':\n",
    "            allowed_min = 150\n",
    "            allowed_max = 360\n",
    "        elif varname == 'pr':\n",
    "            allowed_min = 0\n",
    "            allowed_max = 3000\n",
    "        else:\n",
    "            raise ValueError(f'Variable name not recognized: {varname}\\nin file: {fp}')\n",
    "\n",
    "        assert (vmin >= allowed_min).item() is True, (\n",
    "            f\"min value {vmin} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "        assert (vmax <= allowed_max).item() is True, (\n",
    "            f\"max value {vmax} outside allowed range [{allowed_min}, {allowed_max}] \"\n",
    "            f\"for {varname} in {fp}\"\n",
    "        )\n",
    "\n",
    "        if check_dtr:\n",
    "            assert (min_dtr >= 0).item() is True, (\n",
    "                f\"DTR not greater than zero - min DTR: {min_dtr.item()} in {fp}\"\n",
    "            )\n",
    "\n",
    "\n",
    "@rhgu.block_globals\n",
    "def copy_and_validate(\n",
    "    source_fp,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    "):\n",
    "\n",
    "    spec = get_spec_from_input_fp(source_fp, output_version=output_version)\n",
    "    flipped_fp = spec['flipped_fp']\n",
    "    output_fp = spec['output_fp']\n",
    "    model = spec['model']\n",
    "    scenario = spec['scenario']\n",
    "\n",
    "    fs = fsspec.filesystem('gs', timeout=360, cache_timeout=360, requests_timeout=360, read_timeout=360, conn_timeout=360, token='/opt/gcsfuse_tokens/impactlab-data.json')\n",
    "\n",
    "    if fs.exists(output_fp):\n",
    "        if overwrite:\n",
    "            fs.remove(output_fp, recursive=True)\n",
    "\n",
    "        else:\n",
    "            if deep_copy_check:\n",
    "                dirs = list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps])\n",
    "                if pbar:\n",
    "                    dirs = tqdm(dirs)\n",
    "\n",
    "                for d, f in dirs:\n",
    "                    src = flipped_fp[:5] + os.path.join(d, f)\n",
    "                    dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "                    assert '..' not in dst\n",
    "                    src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "                    for i in range(5):\n",
    "                        try:\n",
    "                            assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                            break\n",
    "                        except (FileNotFoundError, AssertionError):\n",
    "                            if i == 4:\n",
    "                                raise\n",
    "\n",
    "                            fs.rm(dst)\n",
    "                            fs.copy(src, dst)\n",
    "\n",
    "            if check:\n",
    "                try:\n",
    "                    validate_outputs(\n",
    "                        output_fp,\n",
    "                        check_dtr=(check_dtr and (spec['variable'] == 'tasmin')),\n",
    "                    )\n",
    "                    return\n",
    "                except (\n",
    "                    AssertionError,\n",
    "                    FileNotFoundError,\n",
    "                    ValueError,\n",
    "                    IOError,\n",
    "                    xr.coding.times.OutOfBoundsDatetime,\n",
    "                    OverflowError,\n",
    "                ):\n",
    "                    if overwrite_on_failure:\n",
    "                        fs.rm(output_fp, recursive=True)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            elif quick_check_and_retry:\n",
    "                try:\n",
    "                    validate_outputs(output_fp, quick=True)\n",
    "                    return\n",
    "                except (\n",
    "                    OverflowError,\n",
    "                    IOError,\n",
    "                    zarr.errors.GroupNotFoundError,\n",
    "                    FileNotFoundError,\n",
    "                    AssertionError,\n",
    "                    ValueError,\n",
    "                ):\n",
    "                    pass\n",
    "\n",
    "                fs.rm(output_fp, recursive=True)\n",
    "            else:\n",
    "                return\n",
    "\n",
    "    print(f'copying:\\n\\tsrc:\\t{flipped_fp}\\n\\tdst:\\t{output_fp}')\n",
    "    fs.copy(flipped_fp, output_fp, recursive=True, batch_size=1000)\n",
    "\n",
    "    if deep_copy_check:\n",
    "        for d, f in list([(d, f) for d, dirs, fps in fs.walk(flipped_fp) for f in fps]):\n",
    "            src = flipped_fp[:5] + os.path.join(d, f)\n",
    "            dst = os.path.join(output_fp, os.path.relpath(src, flipped_fp))\n",
    "            assert '..' not in dst\n",
    "            src_hash = fs.stat(src)['md5Hash']\n",
    "\n",
    "            for i in range(5):\n",
    "                try:\n",
    "                    assert (src_hash == fs.stat(dst)['md5Hash'])\n",
    "                    break\n",
    "                except (FileNotFoundError, AssertionError):\n",
    "                    if i == 4:\n",
    "                        raise\n",
    "\n",
    "                    fs.rm(dst)\n",
    "                    fs.copy(src, dst)\n",
    "\n",
    "    if check:\n",
    "        validate_outputs(\n",
    "            output_fp,\n",
    "            check_dtr=(check_dtr and (spec['variable'] == 'tasmin')),\n",
    "        )\n",
    "    elif quick_check_and_retry:\n",
    "        validate_outputs(output_fp, quick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, cluster = rhgk.get_giant_cluster()\n",
    "cluster.scale(60)\n",
    "\n",
    "MAX_MEM = '16GB' # for standard cluster\n",
    "\n",
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('https://compute.impactlab.org' + cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "930dc8254dab4118b8dfab059011d154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skipping BCC-CSM2-MR historical - output already exists\n",
      "skipping BCC-CSM2-MR ssp245 - output already exists\n",
      "skipping BCC-CSM2-MR ssp370 - output already exists\n",
      "skipping FGOALS-g3 historical - output already exists\n",
      "skipping FGOALS-g3 ssp245 - output already exists\n",
      "skipping FGOALS-g3 ssp370 - output already exists\n",
      "skipping ACCESS-ESM1-5 historical - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp245 - output already exists\n",
      "skipping ACCESS-ESM1-5 ssp370 - output already exists\n",
      "skipping ACCESS-CM2 historical - output already exists\n",
      "skipping ACCESS-CM2 ssp245 - output already exists\n",
      "skipping ACCESS-CM2 ssp370 - output already exists\n",
      "skipping INM-CM4-8 historical - output already exists\n",
      "skipping INM-CM4-8 ssp245 - output already exists\n",
      "skipping INM-CM4-8 ssp370 - output already exists\n",
      "skipping INM-CM5-0 historical - output already exists\n",
      "skipping INM-CM5-0 ssp245 - output already exists\n",
      "skipping INM-CM5-0 ssp370 - output already exists\n",
      "skipping MIROC-ES2L historical - output already exists\n",
      "skipping MIROC-ES2L ssp245 - output already exists\n",
      "skipping MIROC-ES2L ssp370 - output already exists\n",
      "skipping MIROC6 historical - output already exists\n",
      "skipping MIROC6 ssp245 - output already exists\n",
      "skipping MIROC6 ssp370 - output already exists\n",
      "skipping NorESM2-LM historical - output already exists\n",
      "skipping NorESM2-LM ssp245 - output already exists\n",
      "skipping NorESM2-LM ssp370 - output already exists\n",
      "skipping NorESM2-MM historical - output already exists\n",
      "skipping NorESM2-MM ssp245 - output already exists\n",
      "skipping NorESM2-MM ssp370 - output already exists\n",
      "skipping GFDL-ESM4 historical - output already exists\n",
      "skipping GFDL-ESM4 ssp245 - output already exists\n",
      "skipping GFDL-ESM4 ssp370 - output already exists\n",
      "skipping GFDL-CM4 historical - output already exists\n",
      "skipping GFDL-CM4 ssp245 - output already exists\n",
      "skipping NESM3 historical - output already exists\n",
      "skipping NESM3 ssp245 - output already exists\n"
     ]
    }
   ],
   "source": [
    "with tqdm(DELIVERY_MODELS) as pbar:\n",
    "    for model in pbar:\n",
    "        for scenario in INPUT_FILE_VERSIONS['file_paths']['tasmin'][model].keys():\n",
    "            \n",
    "            tasmin_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmin'][model][scenario]\n",
    "            tasmin_spec = get_spec_from_input_fp(tasmin_input_fp)\n",
    "\n",
    "            tasmax_input_fp = INPUT_FILE_VERSIONS['file_paths']['tasmax'][model][scenario]\n",
    "            tasmax_spec = get_spec_from_input_fp(tasmax_input_fp)\n",
    "\n",
    "            # comment out this block to reproduce rechunked/flipped data on scratch bucket\n",
    "            if fs.exists(tasmin_spec['output_fp']) and fs.exists(tasmax_spec['output_fp']):\n",
    "                print(f'skipping {model} {scenario} - output already exists')\n",
    "                continue\n",
    "\n",
    "            pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk tasmin'})\n",
    "            rechunk_data('tasmin', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "            pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'rechunk tasmax'})\n",
    "            rechunk_data('tasmax', model, scenario, worker_memory_limit=MAX_MEM)\n",
    "            pbar.set_postfix({'model': model, 'scen': scenario, 'stage': 'flip negative DTR'})\n",
    "            flip_negative_dtr(model, scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasmin_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['tasmin'].items() for s, fp in v.items()]\n",
    "tasmax_files = [fp for m, v in INPUT_FILE_VERSIONS['file_paths']['tasmax'].items() for s, fp in v.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blocking_pbar(futures):\n",
    "    status = {'error': 0, 'killed': 0, 'lost': 0}\n",
    "    with tqdm(dd.as_completed(futures), total=len(futures)) as pbar:\n",
    "        for f in pbar:\n",
    "            if f.status in status.keys():\n",
    "                status[f.status] += 1\n",
    "                pbar.set_postfix(status)\n",
    "\n",
    "    dd.wait(futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy files to final destination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dea024444a49b388685dec91be4bb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasmax_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    tasmax_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(tasmax_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "084118347ade4cff8f99a826eae2d483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasmin_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    tasmin_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=False,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(tasmin_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep copy check\n",
    "Check every file against source to ensure a complete copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2515d35bb2b491f8af67c2b2e5d86ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasmax_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    tasmax_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=True,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(tasmax_futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e47eb3743c89479fa287d00f55d41980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tasmin_futures = client.map(\n",
    "    copy_and_validate,\n",
    "    tasmin_files,\n",
    "    output_version=OUTPUT_VERSION,\n",
    "    check=False,\n",
    "    deep_copy_check=True,\n",
    "    quick_check_and_retry=True,\n",
    "    overwrite=False,\n",
    "    overwrite_on_failure=False,\n",
    "    check_dtr=False,\n",
    "    pbar=False,\n",
    ")\n",
    "\n",
    "blocking_pbar(tasmin_futures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tasmax data in final location\n",
    "Check all tasmax values, including bounds & NAN checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a72c5506a3411bb0d04fdee60ddc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in tqdm(tasmax_files):\n",
    "    copy_and_validate(\n",
    "        f,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=True,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=False,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=False,\n",
    "        pbar=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check tasmin data & DTR in final location\n",
    "Check all tasmin values, including bounds & NAN checks, plus check DTR implied by tasmin & tasmax for positivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8f37aa9faf4a9288225982f2985e35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for f in tqdm(tasmin_files):\n",
    "    copy_and_validate(\n",
    "        f,\n",
    "        output_version=OUTPUT_VERSION,\n",
    "        check=True,\n",
    "        deep_copy_check=False,\n",
    "        quick_check_and_retry=False,\n",
    "        overwrite=False,\n",
    "        overwrite_on_failure=False,\n",
    "        check_dtr=True,\n",
    "        pbar=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.restart()\n",
    "cluster.scale(0)\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs are located in the following directory: gs://downscaled-288ec5ac/outputs\n"
     ]
    }
   ],
   "source": [
    "outfiles = []\n",
    "for f in (tasmin_files + tasmax_files):\n",
    "    outfiles.append(get_spec_from_input_fp(f)['output_fp'])\n",
    "\n",
    "print(f'outputs are located in the following directory: {os.path.commonpath(outfiles).replace(\"gs:/\", \"gs://\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transfer data elsewhere, such as to prep for public delivery or delivery to Catalyst buckets, contact Mike for help with google transfer utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
